{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1defd7",
   "metadata": {},
   "source": [
    "\n",
    "# RAG sobre PDFs locales (sin scraping)\n",
    "\n",
    "Este notebook **lee PDFs de una carpeta** (recursivo), los **trocea**, crea **embeddings**, los guarda en **Chroma**, y permite preguntar con una funci√≥n `ask()`.\n",
    "\n",
    "> Requisitos recomendados (mismo entorno):  \n",
    "> `langchain>=0.2.5`, `langchain-community>=0.2.0`, `langchain-text-splitters>=0.2.0`, `langchain-openai>=0.1.0`  \n",
    "> `chromadb>=0.5.0`, `tiktoken>=0.7.0`, `pypdf>=4`, `python-dotenv>=1.0.1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc39f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (Opcional) Instala dependencias\n",
    "%pip install -U -q langchain langchain-community langchain-openai langchain-text-splitters chromadb tiktoken pypdf python-dotenv pyarrow fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f28c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entorno listo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (OPENAI_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat (por defecto OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Entorno listo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc11fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Carpeta PDFs: C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\n",
      "üóÇÔ∏è Persistencia Chroma: C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\chroma_pdfs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Cambia a tu carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs\")  # Carpeta donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 4\n",
    "\n",
    "# Modelos (ajusta si quieres otros)\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4.1-mini\"\n",
    "\n",
    "# Verificar clave\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Falta OPENAI_API_KEY en variables de entorno o .env\"\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è Persistencia Chroma: {PERSIST_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e260a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Documentos (p√°ginas) cargados: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(pdf_paths, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos m√≠nimos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a5a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunks generados: 217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'Generative AI Deployment and Monitoring.pptx', 'source': 'C:\\\\Trainings\\\\GenIA_trainings\\\\agentic_rag_openai\\\\docs\\\\generative-ai-deployment-and-monitoring.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Lava 600 - Primary\\n#FF3621\\nRGB (255 ,54, 33)\\nC0, M91, Y93, K0\\nNavy 800 - Primary\\n#1B3139\\nRGB (27, 49, 57)\\nC86, M65, Y57, K56\\nMaroon 600\\n#98102A\\nRGB (152, 16, 42)\\nC26, M100, Y84, K24\\nYellow 600\\n#FFAB00\\nRGB (255, 171, 0)\\nC0, M38, Y100, K0\\nGreen 600\\n#00A972\\nRGB (0, 169, 114)\\nC81, M6, Y74, K0\\nBlue 600\\n#2272B4\\nRGB (34, 114,1 80)\\nC86, M52, Y4, K0\\nGray - Navigation\\n#303F47\\nRGB (48, 63, 71)\\nC79, M62, Y54, K44\\nGray ‚Äì Text\\n#5A6F77\\nRGB (90, 111, 119)\\nC68, M47, Y44, K14\\nGray ‚Äì Lines\\n#DCE0E2\\nRGB (220, 224, 226)\\nC12, M7, Y8, K0\\nPrimary palette\\nSecondary palette\\nOat Medium\\n#EEEDE9\\nRGB (238, 237, 233)\\nC6, M4, Y6, K0\\nOat Light\\n#F9F7F4\\nRGB (249, ,247, 244)\\nC1, M2, Y2, K0\\nLava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'Generative AI Deployment and Monitoring.pptx', 'source': 'C:\\\\Trainings\\\\GenIA_trainings\\\\agentic_rag_openai\\\\docs\\\\generative-ai-deployment-and-monitoring.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Lava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nGenerative AI \\nDeployment and \\nMonitoring\\nDatabricks Academy')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\" Chunks generados: {len(chunks)}\")\n",
    "chunks[:2]  # vista r√°pida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce1b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chroma persistido\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "retriever =vectorstore.as_retriever()\n",
    "print(\" Chroma persistido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b10a5e8-4a91-4884-a159-c6dca1077356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",#nombre\n",
    "    \"Busca y devuelve info acerca de tus documentos pdf.\",#Descripcion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a687b80-23ff-4f36-a812-3d42a0ad7355",
   "metadata": {},
   "source": [
    "3 Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb533983-1a85-409e-a6f5-ec141de33775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nLearning Objectives\\n‚óè Describe batch deployment and identify scenarios in which this method \\nis appropriate.\\n‚óè Identify the advantages and disadvantages of deploying a model via \\nbatch processing.\\n‚óè Discuss a typical batch model deployment workÔ¨Çow on Databricks.\\n‚óè Load a logged LM from the model registry and use it for batch inference.\\n\\nLava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nLearning Objectives\\n‚óè Describe batch deployment and identify scenarios in which this method \\nis appropriate.\\n‚óè Identify the advantages and disadvantages of deploying a model via \\nbatch processing.\\n‚óè Discuss a typical batch model deployment workÔ¨Çow on Databricks.\\n‚óè Load a logged LM from the model registry and use it for batch inference.\\n\\nLava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nLearning Objectives\\n‚óè Describe batch deployment and identify scenarios in which this method \\nis appropriate.\\n‚óè Identify the advantages and disadvantages of deploying a model via \\nbatch processing.\\n‚óè Discuss a typical batch model deployment workÔ¨Çow on Databricks.\\n‚óè Load a logged LM from the model registry and use it for batch inference.\\n\\nLava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nLearning Objectives\\n‚óè Describe batch deployment and identify scenarios in which this method \\nis appropriate.\\n‚óè Identify the advantages and disadvantages of deploying a model via \\nbatch processing.\\n‚óè Discuss a typical batch model deployment workÔ¨Çow on Databricks.\\n‚óè Load a logged LM from the model registry and use it for batch inference.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.invoke({\"query\": \"¬øQue es el deployment batch?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae1bb9-99fc-49a0-967b-0b7d6902a83e",
   "metadata": {},
   "source": [
    "3. Generamos la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980e855e-47f3-4b25-82bb-d789008ff98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "response_model=  ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "def genera_query_o_responde(state: MessagesState):\n",
    "    \"\"\"Llama al modelo para generar una respuesta basada en el estado actual.\n",
    "      Dada la pregunta, decidir√° si recupera informaci√≥n usando la herramienta de recuperaci√≥n o simplemente responde al usuario.\"\"\"\n",
    "    print(state)\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c0e946-3c99-463b-8f9c-c94d8b7aadb3",
   "metadata": {},
   "source": [
    "3.1 Lo probamos con una pregunta aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed080d2e-3853-4743-bb42-d371d980d9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '¬øCuanto vale una cortina?'}]}\n",
      "{'messages': [AIMessage(content='El precio de una cortina puede variar ampliamente dependiendo de varios factores, como el material, el tama√±o, el dise√±o, la marca y el lugar donde la compres. Aqu√≠ tienes algunos rangos generales de precios:\\n\\n1. **Cortinas de tela b√°sica**: Pueden costar entre 10 y 50 d√≥lares por panel, dependiendo del tama√±o y la calidad de la tela.\\n\\n2. **Cortinas de lujo o de dise√±ador**: Estas pueden costar desde 100 d√≥lares hasta varios cientos de d√≥lares por panel.\\n\\n3. **Cortinas opacas o blackout**: Suelen ser m√°s caras que las cortinas est√°ndar, con precios que oscilan entre 20 y 100 d√≥lares por panel.\\n\\n4. **Cortinas personalizadas**: Si decides hacer cortinas a medida, el costo puede ser significativamente mayor, dependiendo de las especificaciones.\\n\\n5. **Cortinas de bamb√∫ o madera**: Estas pueden variar entre 30 y 200 d√≥lares, dependiendo del tama√±o y la calidad.\\n\\nEs importante considerar tambi√©n los costos adicionales, como las barras de cortina y los accesorios de montaje. Para obtener un precio exacto, te recomendar√≠a visitar tiendas locales o sitios web de decoraci√≥n del hogar.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 250, 'prompt_tokens': 64, 'total_tokens': 314, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CbKHSJZTn4d80qAlJ4iFfrkhgA5a8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5cdfd7a3-f935-43db-a688-0384f9dcd1fb-0', usage_metadata={'input_tokens': 64, 'output_tokens': 250, 'total_tokens': 314, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCuanto vale una cortina?\"}]}\n",
    "respuesta= genera_query_o_responde(input)\n",
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33a8e09-f029-4adf-bc2d-9f60e89de96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='El precio de una cortina puede variar ampliamente dependiendo de varios factores, como el material, el tama√±o, el dise√±o, la marca y el lugar donde la compres. Aqu√≠ tienes algunos rangos generales de precios:\\n\\n1. **Cortinas de tela b√°sica**: Pueden costar entre 10 y 50 d√≥lares por panel, dependiendo del tama√±o y la calidad de la tela.\\n\\n2. **Cortinas de lujo o de dise√±ador**: Estas pueden costar desde 100 d√≥lares hasta varios cientos de d√≥lares por panel.\\n\\n3. **Cortinas opacas o blackout**: Suelen ser m√°s caras que las cortinas est√°ndar, con precios que oscilan entre 20 y 100 d√≥lares por panel.\\n\\n4. **Cortinas personalizadas**: Si decides hacer cortinas a medida, el costo puede ser significativamente mayor, dependiendo de las especificaciones.\\n\\n5. **Cortinas de bamb√∫ o madera**: Estas pueden variar entre 30 y 200 d√≥lares, dependiendo del tama√±o y la calidad.\\n\\nEs importante considerar tambi√©n los costos adicionales, como las barras de cortina y los accesorios de montaje. Para obtener un precio exacto, te recomendar√≠a visitar tiendas locales o sitios web de decoraci√≥n del hogar.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 250, 'prompt_tokens': 64, 'total_tokens': 314, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CbKHSJZTn4d80qAlJ4iFfrkhgA5a8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5cdfd7a3-f935-43db-a688-0384f9dcd1fb-0', usage_metadata={'input_tokens': 64, 'output_tokens': 250, 'total_tokens': 314, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cogemos el ultimo mensaje\n",
    "respuesta[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5caed662-dbe2-443b-a904-402f33d59058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "El precio de una cortina puede variar ampliamente dependiendo de varios factores, como el material, el tama√±o, el dise√±o, la marca y el lugar donde la compres. Aqu√≠ tienes algunos rangos generales de precios:\n",
      "\n",
      "1. **Cortinas de tela b√°sica**: Pueden costar entre 10 y 50 d√≥lares por panel, dependiendo del tama√±o y la calidad de la tela.\n",
      "\n",
      "2. **Cortinas de lujo o de dise√±ador**: Estas pueden costar desde 100 d√≥lares hasta varios cientos de d√≥lares por panel.\n",
      "\n",
      "3. **Cortinas opacas o blackout**: Suelen ser m√°s caras que las cortinas est√°ndar, con precios que oscilan entre 20 y 100 d√≥lares por panel.\n",
      "\n",
      "4. **Cortinas personalizadas**: Si decides hacer cortinas a medida, el costo puede ser significativamente mayor, dependiendo de las especificaciones.\n",
      "\n",
      "5. **Cortinas de bamb√∫ o madera**: Estas pueden variar entre 30 y 200 d√≥lares, dependiendo del tama√±o y la calidad.\n",
      "\n",
      "Es importante considerar tambi√©n los costos adicionales, como las barras de cortina y los accesorios de montaje. Para obtener un precio exacto, te recomendar√≠a visitar tiendas locales o sitios web de decoraci√≥n del hogar.\n"
     ]
    }
   ],
   "source": [
    "respuesta[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad685ee3-011c-458b-a2d3-94505679fa8c",
   "metadata": {},
   "source": [
    "3.2  Hacer una pregunta que requiera una busqueda semantica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d8f121d-f473-4dba-b2b3-2ed84feecdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': '¬øQue es un deployment tipo batch?, a√±ade la informacion que necesites'}]}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_blog_posts (call_NvqzdV1Ybi14WtwgGMTrQeNJ)\n",
      " Call ID: call_NvqzdV1Ybi14WtwgGMTrQeNJ\n",
      "  Args:\n",
      "    query: deployment tipo batch\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # \"content\": \"Busca en la informacion proporcionada y no inventes¬øQue es un deployment tipo batch?\",\n",
    "            \"content\": \"¬øQue es un deployment tipo batch?, a√±ade la informacion que necesites\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "genera_query_o_responde(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20746669-1361-4c1e-b3cf-c5f78a09fa89",
   "metadata": {},
   "source": [
    "4. Vemos la relevancia del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e56baa-778d-44a5-9cf2-a980c9984b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"Eres un evaluador que determina la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \"\n",
    "    \"Aqu√≠ tienes el documento recuperado: \\n\\n {context} \\n\\n\"\n",
    "    \"Aqu√≠ tienes la pregunta del usuario: {question} \\n\"\n",
    "    \"Si el documento contiene palabra(s) clave o significado sem√°ntico relacionado con la pregunta del usuario, calif√≠calo como relevante. \\n\"\n",
    "    \"Da una puntuaci√≥n binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Califica los documentos utilizando una puntuaci√≥n binaria para comprobar su relevancia\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Puntuaci√≥n : 'si' si es relevante, o 'no' si no lo es\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"genera_respuesta\", \"rescribir_question\"]:\n",
    "    \"\"\"Determina si los documentos recuperados son relevantes para la pregunta.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    print (\"question:\",question)\n",
    "    print(\"context: \", context)\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "    print (\"score :\", score)\n",
    "    if score == \"si\":\n",
    "        return \"genera_respuesta\"\n",
    "    else:\n",
    "        return \"rescribir_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf0b67-84a4-4c97-a5f9-3a5d82963b8f",
   "metadata": {},
   "source": [
    "4.1 Ejecutar con una respuesta irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45f88aad-acf8-4e31-b9f2-4c50775a81b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: {'messages': [HumanMessage(content='¬øQue tiempo hace en Alcante?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={}, response_metadata={}, tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': '¬øQue es un deployment batch?'}, 'id': '1', 'type': 'tool_call'}]), ToolMessage(content='Son las 10 de la ma√±ana', tool_call_id='1')]}\n",
      "question: ¬øQue tiempo hace en Alcante?\n",
      "context:  Son las 10 de la ma√±ana\n",
      "score : no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rescribir_question'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simulamos la respuesta de la tool mediante mensajes\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQue tiempo hace en Alcante?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es un deployment batch?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Son las 10 de la ma√±ana\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "print (\"input:\" , input)\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501f40e-60a3-4c2b-a436-4416bd49a217",
   "metadata": {},
   "source": [
    "4.2 Coomprobar que el documento/respuesta relevante lo clasifica como tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "726ab5ca-a176-4309-91ce-c6a26f9f2bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: ¬øC√≥mo Samsung a adelanto Google?\n",
      "context:  El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks.\n",
      "score : no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rescribir_question'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                # \"content\": \"¬øC√≥mo Samsung a adelanto Google?\", comprobar que la rescribe\n",
    "                \"content\": \"¬øC√≥mo Samsung a adelanto Google?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es batch en eldocumento?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
