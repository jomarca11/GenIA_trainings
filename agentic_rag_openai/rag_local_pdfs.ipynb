{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1defd7",
   "metadata": {},
   "source": [
    "\n",
    "# RAG sobre PDFs locales (sin scraping)\n",
    "\n",
    "Este notebook **lee PDFs de una carpeta** (recursivo), los **trocea**, crea **embeddings**, los guarda en **Chroma**, y permite preguntar con una funci√≥n `ask()`.\n",
    "\n",
    "> Requisitos recomendados (mismo entorno):  \n",
    "> `langchain>=0.2.5`, `langchain-community>=0.2.0`, `langchain-text-splitters>=0.2.0`, `langchain-openai>=0.1.0`  \n",
    "> `chromadb>=0.5.0`, `tiktoken>=0.7.0`, `pypdf>=4`, `python-dotenv>=1.0.1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc39f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# (Opcional) Instala dependencias\n",
    "%pip install -U -q langchain langchain-community langchain-openai langchain-text-splitters chromadb tiktoken pypdf python-dotenv pyarrow fastparquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f28c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entorno listo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (OPENAI_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat (por defecto OpenAI)\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Entorno listo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc11fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Carpeta PDFs: C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\n",
      "üóÇÔ∏è Persistencia Chroma: C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\chroma_pdfs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Cambia a tu carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs\")  # Carpeta donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "TOP_K = 4\n",
    "\n",
    "# Modelos (ajusta si quieres otros)\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "CHAT_MODEL = \"gpt-4.1-mini\"\n",
    "\n",
    "# Verificar clave\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Falta OPENAI_API_KEY en variables de entorno o .env\"\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è Persistencia Chroma: {PERSIST_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e260a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Documentos (p√°ginas) cargados: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(pdf_paths, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos m√≠nimos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a5a885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunks generados: 217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'Generative AI Deployment and Monitoring.pptx', 'source': 'C:\\\\Trainings\\\\GenIA_trainings\\\\agentic_rag_openai\\\\docs\\\\generative-ai-deployment-and-monitoring.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Lava 600 - Primary\\n#FF3621\\nRGB (255 ,54, 33)\\nC0, M91, Y93, K0\\nNavy 800 - Primary\\n#1B3139\\nRGB (27, 49, 57)\\nC86, M65, Y57, K56\\nMaroon 600\\n#98102A\\nRGB (152, 16, 42)\\nC26, M100, Y84, K24\\nYellow 600\\n#FFAB00\\nRGB (255, 171, 0)\\nC0, M38, Y100, K0\\nGreen 600\\n#00A972\\nRGB (0, 169, 114)\\nC81, M6, Y74, K0\\nBlue 600\\n#2272B4\\nRGB (34, 114,1 80)\\nC86, M52, Y4, K0\\nGray - Navigation\\n#303F47\\nRGB (48, 63, 71)\\nC79, M62, Y54, K44\\nGray ‚Äì Text\\n#5A6F77\\nRGB (90, 111, 119)\\nC68, M47, Y44, K14\\nGray ‚Äì Lines\\n#DCE0E2\\nRGB (220, 224, 226)\\nC12, M7, Y8, K0\\nPrimary palette\\nSecondary palette\\nOat Medium\\n#EEEDE9\\nRGB (238, 237, 233)\\nC6, M4, Y6, K0\\nOat Light\\n#F9F7F4\\nRGB (249, ,247, 244)\\nC1, M2, Y2, K0\\nLava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Google', 'creationdate': '', 'title': 'Generative AI Deployment and Monitoring.pptx', 'source': 'C:\\\\Trainings\\\\GenIA_trainings\\\\agentic_rag_openai\\\\docs\\\\generative-ai-deployment-and-monitoring.pdf', 'total_pages': 98, 'page': 0, 'page_label': '1'}, page_content='Lava 500\\n#FF5F46\\nRGB (255, 95, 70)\\nC0, M78, Y79, K0\\nNavy 900\\n#0B2026\\nRGB (11, 32, 38)\\nC86, M67, Y61,  K71\\n¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \\nIceberg logo are trademarks of the Apache Software Foundation.\\nGenerative AI \\nDeployment and \\nMonitoring\\nDatabricks Academy')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\" Chunks generados: {len(chunks)}\")\n",
    "chunks[:2]  # vista r√°pida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce1b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chroma persistido\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "#vectordb.persist()\n",
    "print(\" Chroma persistido\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9a9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ask() actualizado. Usa: answer, docs = ask('tu pregunta')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "llm = ChatOpenAI(model=CHAT_MODEL)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Eres un asistente experto. Responde bas√°ndote EXCLUSIVAMENTE en el contexto entregado. \"\n",
    "    \"Si la respuesta no est√° en las fuentes, di con claridad que no est√° disponible.\"\n",
    ")\n",
    "\n",
    "def _retrieve(query: str):\n",
    "    \"\"\"Funci√≥n compatible: usa .invoke() y, si no existe, cae a .get_relevant_documents().\"\"\"\n",
    "    if hasattr(retriever, \"invoke\"):\n",
    "        return retriever.invoke(query)\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def ask(query: str):\n",
    "    # 1) Recuperaci√≥n\n",
    "    docs = _retrieve(query)\n",
    "\n",
    "    # 2) Componer contexto\n",
    "    context = \"\\n\\n---\\n\\n\".join(\n",
    "        [f\"[Fuente: {d.metadata.get('source','desconocida')}]\\n{(d.page_content or '')[:2000]}\" for d in docs]\n",
    "    )\n",
    "\n",
    "    # 3) LLM\n",
    "    messages = [\n",
    "        SystemMessage(content=SYSTEM_PROMPT),\n",
    "        HumanMessage(content=f\"Pregunta: {query}\\n\\nContexto:\\n{context}\")\n",
    "    ]\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content, docs\n",
    "\n",
    "print(\"‚úÖ ask() actualizado. Usa: answer, docs = ask('tu pregunta')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3742408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Respuesta: El contexto proporcionado hace referencia a \"batch deployment\" (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es \"batch\". Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks.\n",
      "\n",
      "Por lo tanto, bas√°ndonos en el contexto, \"batch\" se refiere al procesamiento o despliegue de modelos mediante la ejecuci√≥n de inferencias o tareas en grupos o lotes de datos, en lugar de procesar datos individualmente en tiempo real. Este m√©todo suele usarse en casos donde no se requiere respuesta inmediata y es m√°s eficiente procesar m√∫ltiples elementos juntos.\n",
      "\n",
      "En resumen, \"batch\" en el documento implica un despliegue y procesamiento por lotes para realizar inferencias de modelos sobre conjuntos de datos agrupados, gestionado t√≠picamente en plataformas como Databricks.\n",
      "\n",
      "üìé Fuentes:\n",
      "1. C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\\generative-ai-deployment-and-monitoring.pdf\n",
      "2. C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\\generative-ai-deployment-and-monitoring.pdf\n",
      "3. C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\\generative-ai-deployment-and-monitoring.pdf\n",
      "4. C:\\Trainings\\GenIA_trainings\\agentic_rag_openai\\docs\\generative-ai-deployment-and-monitoring.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prueba r√°pida (cambia la pregunta a algo que exista en tus PDFs)\n",
    "question = \"¬øQue es batch en eldocumento?\"\n",
    "answer, support_docs = ask(question)\n",
    "print(\"üß† Respuesta:\", answer)\n",
    "print(\"\\nüìé Fuentes:\")\n",
    "for i, d in enumerate(support_docs, 1):\n",
    "    print(f\"{i}.\", d.metadata.get(\"source\", \"desconocida\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5db71",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Consejos\n",
    "- Puedes re‚Äëejecutar solo las celdas 4‚Üí8 para regenerar el vector store tras a√±adir PDFs.\n",
    "- Si quieres **reconstruir** Chroma desde cero, elimina la carpeta `chroma_pdfs` antes de ejecutar la celda 6.\n",
    "- Para usar otro proveedor de embeddings/LLM, reemplaza `OpenAIEmbeddings` / `ChatOpenAI` por el wrapper correspondiente.\n",
    "- Si tienes PDFs escaneados (im√°genes), necesitar√°s OCR (por ejemplo, `pytesseract` + `pdf2image`) antes de este flujo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
