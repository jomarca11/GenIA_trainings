{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298d5b29",
   "metadata": {},
   "source": [
    "## 1. Instalamos dependencias y librer√≠as\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 2-3 minutos**\n",
    "\n",
    "### ¬øQu√© vamos a hacer?\n",
    "Instalamos las librer√≠as necesarias para:\n",
    "- Leer PDFs\n",
    "- Crear embeddings (representaciones num√©ricas del texto)\n",
    "- Usar el modelo Gemini de Google\n",
    "- Construir un grafo de estados (agentico workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q langchain>=0.2.5 langchain-community>=0.2.0 langchain-text-splitters>=0.2.0 langchain-google-genai>=0.0.10 chromadb>=0.5.0 tiktoken>=0.7.0 pypdf>=4 python-dotenv>=1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada62beb",
   "metadata": {},
   "source": [
    "## 2. Importamos las librer√≠as necesarias\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### Desglose de importaciones:\n",
    "- **PyPDFLoader**: Lee PDFs\n",
    "- **RecursiveCharacterTextSplitter**: Divide el texto en chunks (trozos)\n",
    "- **Chroma**: Vector database para almacenar embeddings\n",
    "- **GoogleGenerativeAIEmbeddings**: Convierte texto en vectores\n",
    "- **ChatGoogleGenerativeAI**: Modelo de lenguaje Gemini\n",
    "- **tqdm**: Barra de progreso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c68db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (GOOGLE_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat con Google Gemini. Teneis las pistas por el notebook. Hasta aqui puedo leer\n",
    "from langchain_google_genai import <FILL_IN>, <FILL_IN>\n",
    "\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c8c5e",
   "metadata": {},
   "source": [
    "## 3. Configuramos el entorno\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3-5 minutos**\n",
    "\n",
    "### Conceptos clave:\n",
    "- **CHUNK_SIZE**: Tama√±o de cada fragmento de texto (ej: 1000 caracteres)\n",
    "- **CHUNK_OVERLAP**: Solapamiento entre chunks (ej: 200 caracteres) para mantener contexto\n",
    "- **TOP_K**: N√∫mero de documentos m√°s relevantes a recuperar\n",
    "- **Modelos**: Necesitas elegir embeddings y modelo de chat\n",
    "\n",
    "üí° **Tip**: Valores t√≠picos:\n",
    "- CHUNK_SIZE: 500-2000\n",
    "- CHUNK_OVERLAP: 100-400 (~20% del CHUNK_SIZE)\n",
    "- Embedding: \"models/text-embedding-004\"\n",
    "- Chat: \"gemini-2.5-flash\" o \"gemini-1.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b518eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs_gemini\")  # Donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# <FILL_IN> Completa estos valores bas√°ndote en los tips\n",
    "CHUNK_SIZE = <FILL_IN>  # Tama√±o de cada chunk (trozo) de texto. Rango: 500-2000\n",
    "CHUNK_OVERLAP = <FILL_IN>  # Solapamiento entre chunks. Recomendado: ~20% del CHUNK_SIZE\n",
    "TOP_K = 4  # N√∫mero de documentos recuperados (mant√©n este valor)\n",
    "\n",
    "# Modelos de Gemini\n",
    "EMBEDDING_MODEL = \"<FILL_IN>\"  # Modelo de embeddings (usa: \"models/text-embedding-004\")\n",
    "CHAT_MODEL = \"<FILL_IN>\"  # Modelo de chat (prueba: \"gemini-2.5-flash\")\n",
    "\n",
    "# Verificar clave - SIN ESTO, NO FUNCIONA\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"‚ùå Falta GOOGLE_API_KEY en variables de entorno o .env\"\n",
    "print(f\"‚úÖ GOOGLE_API_KEY detectada\")\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è  Persistencia Chroma: {PERSIST_DIR.resolve()}\")\n",
    "print(f\"üîß Configuraci√≥n: CHUNK_SIZE={CHUNK_SIZE}, OVERLAP={CHUNK_OVERLAP}, TOP_K={TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670fbb5",
   "metadata": {},
   "source": [
    "## 4. Cargamos los PDFs\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos** (seg√∫n cantidad de PDFs)\n",
    "\n",
    "### ¬øQu√© ocurre aqu√≠?\n",
    "1. Lee todos los PDFs de la carpeta `./docs`\n",
    "2. A√±ade metadatos (origen del documento) a cada p√°gina\n",
    "3. Retorna lista de documentos cargados\n",
    "\n",
    "üéØ **Objetivo**: Tener una lista de documentos listos para procesar\n",
    "\n",
    "üí° **Tip de debugging**: Si no ves docs, verifica que:\n",
    "- Los PDFs est√°n en `./docs`\n",
    "- Los PDFs no est√°n corruptos\n",
    "- Tienes permisos de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    \"\"\"Carga todos los PDFs de una carpeta de forma recursiva.\"\"\"\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(<FILL_IN>, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665b280",
   "metadata": {},
   "source": [
    "## 5. Chunking.\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1-2 minutos**\n",
    "\n",
    "### ¬øPor qu√© hacer chunking?\n",
    "- Los modelos tienen l√≠mite de tokens (palabras)\n",
    "- Dividir en trozos permite recuperar partes relevantes\n",
    "- El solapamiento preserva contexto entre chunks\n",
    "\n",
    "### Par√°metros de recursi√≥n:\n",
    "- `separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]` ‚Üí Intenta respetar p√°rrafos, luego l√≠neas, luego palabras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c86eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Crea el splitter con los MISMOS par√°metros que definiste arriba (CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "# Usa las variables CHUNK_SIZE y CHUNK_OVERLAP que ya configuraste\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=<FILL_IN>,\n",
    "    chunk_overlap=<FILL_IN>,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"‚úÇÔ∏è  Chunks generados: {len(chunks)}\")\n",
    "print(f\"üìà Ratio de expansi√≥n: {len(chunks)} chunks de {len(raw_docs)} documentos\")\n",
    "if chunks:\n",
    "    print(f\"üìù Ejemplo de primer chunk (primeros 200 caracteres):\\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52335f",
   "metadata": {},
   "source": [
    "## 6. Crear Embeddings y Vector Store\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-15 minutos** (seg√∫n cantidad de chunks)\n",
    "\n",
    "### Conceptos:\n",
    "- **Embeddings**: Convertir texto a n√∫meros (vectores)\n",
    "  - El modelo de embeddings crea una representaci√≥n num√©rica\n",
    "  - Textos similares tienen vectores cercanos\n",
    "- **Vector Store (Chroma)**: Base de datos de vectores\n",
    "  - Almacena chunks + sus embeddings\n",
    "  - Permite b√∫squeda sem√°ntica r√°pida\n",
    "- **Retriever**: Interfaz para recuperar documentos similares\n",
    "\n",
    "üí° **Pista**: Este paso es intensivo. Es normal esperar.\n",
    "\n",
    "üéØ **Prueba de checkpoint**: El vector store debe persistirse en `./chroma_pdfs_gemini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa307b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Crea los embeddings con el modelo configurado (usa EMBEDDING_MODEL)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=<FILL_IN>)\n",
    "print(\"‚úÖ Embeddings inicializados\")\n",
    "\n",
    "# <FILL_IN> Crea el vector store con Chroma\n",
    "# Necesitas: documents=chunks, embedding=embeddings, persist_directory\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=<FILL_IN>,\n",
    "    embedding=<FILL_IN>,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "print(f\"‚úÖ Vector store creado con {len(chunks)} chunks\")\n",
    "\n",
    "# <FILL_IN> Crea el retriever a partir del vector store\n",
    "# Usa: vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "retriever = <FILL_IN>\n",
    "print(\"‚úÖ Retriever creado y listo para b√∫squedas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c0437",
   "metadata": {},
   "source": [
    "## 7. Creamos la Tool (Retriever) para el agente\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 2 minutos**\n",
    "\n",
    "### ¬øQu√© es una Tool?\n",
    "Una herramienta que el agente puede usar durante su ejecuci√≥n:\n",
    "- **Nombre**: Identificador √∫nico\n",
    "- **Descripci√≥n**: Qu√© hace (el modelo la lee para decidir si usarla)\n",
    "- **Funci√≥n**: El retriever que implementa la b√∫squeda\n",
    "\n",
    "üí° **Pista**: Una buena descripci√≥n ayuda al modelo a saber cu√°ndo usar esta herramienta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# <FILL_IN> Crea la retriever tool\n",
    "# Par√°metros:\n",
    "#  1. retriever (el objeto retriever que creaste)\n",
    "#  2. nombre (string, ej: \"search_documents\")\n",
    "#  3. descripci√≥n (string, ej: \"Busca informaci√≥n en los documentos sobre...\")\n",
    "retriever_tool = create_retriever_tool(\n",
    "    <FILL_IN>,  # retriever\n",
    "    \"<FILL_IN>\",  # nombre de la tool (ej: \"pdf_search\")\n",
    "    \"<FILL_IN>\",  # descripci√≥n (ej: \"Busca informaci√≥n en los PDFs cargados\")\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retriever tool '{retriever_tool.name}' creada\")\n",
    "print(f\"   Descripci√≥n: {retriever_tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77a63c",
   "metadata": {},
   "source": [
    "### 7.1 Test de la tool\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 2 minutos**\n",
    "\n",
    "**Objetivo**: Verificar que la tool funciona correctamente\n",
    "- Ejecuta una b√∫squeda real contra tu vector store\n",
    "- Observa qu√© documentos se recuperan\n",
    "- Valida que son relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba la tool con una pregunta sobre tus PDFs\n",
    "# Ejemplo: \"Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue es un deployment tipo batch?\"\n",
    "resultado = retriever_tool.invoke({\"query\": \"<FILL_IN>\"})\n",
    "print(\"\\nüìÑ Documentos recuperados:\")\n",
    "print(\"-\" * 60)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627ae19",
   "metadata": {},
   "source": [
    "## 8. Nodo: Genera query o responde directamente\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5 minutos**\n",
    "\n",
    "### Flujo del agente (agentico loop):\n",
    "1. **Este nodo**: Recibe la pregunta del usuario\n",
    "2. Decide: ¬øNecesito buscar docs o puedo responder directamente?\n",
    "3. Si usa la tool ‚Üí pasa a retriever\n",
    "4. Si responde ‚Üí termina\n",
    "\n",
    "### Conceptos clave:\n",
    "- **bind_tools**: Conecta herramientas al modelo\n",
    "- **MessagesState**: Estado que mantiene el historial de mensajes\n",
    "- **ToolUse**: Cuando el modelo elige usar una herramienta\n",
    "\n",
    "üí° **Pista**: Es el primer nodo del grafo agentico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f91b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Instancia el modelo de chat con el modelo configurado\n",
    "# temperature: 0=determinista, 1=creativo. Para RAG: usa 0-0.5\n",
    "response_model = ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "print(f\"‚úÖ Modelo de chat {CHAT_MODEL} configurado\")\n",
    "\n",
    "def genera_query_o_responde(state: MessagesState):\n",
    "    \"\"\"Nodo 1: Decide si recuperar informaci√≥n o responder directamente.\"\"\"\n",
    "    # <FILL_IN> Conecta la tool al modelo usando bind_tools\n",
    "    # Par√°metros:\n",
    "    #  - [lista de tools]\n",
    "    #  - Luego .invoke(state[\"messages\"])\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([<FILL_IN>]).invoke(state[<FILL_IN>])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d3404",
   "metadata": {},
   "source": [
    "### 8.1 Test: Pregunta que no necesita b√∫squeda\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "**Esperado**: El modelo responde directamente sin usar la tool\n",
    "- Pregunta gen√©rica (ej: \"¬øCu√°l es la capital de Francia?\")\n",
    "- Observa que NO hay tool_use en la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba con una pregunta general (NO sobre tus PDFs)\n",
    "# Ej: \"Expl√≠came qu√© es RAG\", \"¬øCu√°l es 2+2?\"\n",
    "input_test = {\"messages\": [{\"role\": \"user\", \"content\": \"<FILL_IN>\"}]}\n",
    "print(\"\\nüß™ Test 1: Pregunta sin necesidad de b√∫squeda\\n\")\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()\n",
    "print(\"\\n‚úÖ Observa que NO hay 'tool_use' en la respuesta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903e525",
   "metadata": {},
   "source": [
    "### 8.2 Test: Pregunta que requiere b√∫squeda sem√°ntica\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "**Esperado**: El modelo usa la tool para buscar documentos\n",
    "- Pregunta sobre contenido de tus PDFs\n",
    "- Observa que S√ç hay tool_use con el nombre de tu tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f125ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba con una pregunta sobre tus PDFs\n",
    "# Ej: \"¬øQu√© se menciona sobre...?\", \"Resumime el contenido sobre...\"\n",
    "input_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"<FILL_IN>\", # rol que comienza la accion..empieza  por \"u\" y termina por \"ser\". No puedo dar mas pistas\n",
    "            \"content\": \"<FILL_IN>\",  # Pregunta sobre el contenido de tus PDFs. Buscad en los pdfs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(\"\\nüß™ Test 2: Pregunta que REQUIERE b√∫squeda\\n\")\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()\n",
    "print(\"\\n‚úÖ Observa que S√ç hay 'tool_use' (tu tool debe estar en tool_calls)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e8d4d",
   "metadata": {},
   "source": [
    "## 9. Nodo: Evaluar relevancia de documentos\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5 minutos**\n",
    "\n",
    "### ¬øPor qu√© evaluar relevancia?\n",
    "- No siempre la b√∫squeda sem√°ntica recupera docs relevantes\n",
    "- Control de calidad: rechazar docs no pertinentes\n",
    "- Bifurcaci√≥n: docs relevantes ‚Üí responder, docs irrelevantes ‚Üí reescribir pregunta\n",
    "\n",
    "### GradeDocuments (Pydantic):\n",
    "- Estructura de datos con score binario (\"si\"/\"no\")\n",
    "- Ayuda a parsear la respuesta del modelo\n",
    "\n",
    "üí° **Concepto**: Los modelos pueden ser instructores, pero tambi√©n evaluadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Instancia el modelo grader (puede ser el mismo que response_model)\n",
    "# temperature bajo para evaluaci√≥n coherente\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"Eres un evaluador que determina la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \"\n",
    "    \"Aqu√≠ tienes el documento recuperado: \\n\\n {context} \\n\\n\"\n",
    "    \"Aqu√≠ tienes la pregunta del usuario: {question} \\n\"\n",
    "    \"Si el documento contiene palabra(s) clave o significado sem√°ntico relacionado con la pregunta del usuario, calif√≠calo como relevante. \\n\"\n",
    "    \"Da una puntuaci√≥n binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Califica los documentos utilizando una puntuaci√≥n binaria para comprobar su relevancia\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Puntuaci√≥n : 'si' si es relevante, o 'no' si no lo es\"\n",
    "    )\n",
    "\n",
    "grader_model = ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "\n",
    "\n",
    "def grade_documents(state: MessagesState) -> Literal[\"genera_respuesta\", \"rescribir_question\"]:\n",
    "    \"\"\"Nodo 2: Eval√∫a si los documentos recuperados son relevantes.\n",
    "    \n",
    "    Retorna:\n",
    "    - \"genera_respuesta\" si docs son relevantes\n",
    "    - \"rescribir_question\" si no lo son\n",
    "    \"\"\"\n",
    "    print(\"\\n‚è≥ Evaluando relevancia de documentos...\")\n",
    "    \n",
    "    # <FILL_IN> Extrae la pregunta (√∫ltimo mensaje de usuario)\n",
    "    # Pista: es el primer elemento de state[\"messages\"][].content\n",
    "    question = <FILL_IN>\n",
    "    \n",
    "    # <FILL_IN> Extrae el contexto (√∫ltimo ToolMessage con documentos)\n",
    "    # Pista: Busca en state[\"messages\"] un mensaje que venga del retriever. \n",
    "    # Es el ultimo mensaje del elemento state[\"messages][].content\n",
    "    context = <FILL_IN>\n",
    "    \n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "    print(f\"üìä Score de relevancia: {score}\")\n",
    "    \n",
    "    if score == \"si\":\n",
    "        print(\"‚úÖ Docs relevantes ‚Üí generando respuesta\")\n",
    "        return \"genera_respuesta\"\n",
    "    else:\n",
    "        print(\"‚ùå Docs no relevantes ‚Üí reescribiendo pregunta\")\n",
    "        return \"rescribir_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426988fd-2ef8-4ba5-b35e-fb7c7e641bb0",
   "metadata": {},
   "source": [
    "#### 9.1 Comprobamos con una respuesta irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ded288-dd85-4137-a7bc-0cf994c178d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulamos la respuesta de la tool mediante mensajes\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\",#Pregunta que no tenga ningun sentido\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es un deployment batch?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Son las 10 de la ma√±ana\", \"tool_call_id\": \"1\"}, #Esta respuesta que devuelve la  tool y on tiene que ver con la pregunta\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de19d3-5fc1-4e92-a12b-2618fb24e5c8",
   "metadata": {},
   "source": [
    "#### 9.2 Comprobar que el documento/respuesta relevante lo clasifica como tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae401e55-debf-4857-ab85-c3908970169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "               \n",
    "                \"content\": \"¬øQue es un deployment batch en el documento?\", \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es un deployment batch en el documento?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                ##El contenido de la respuesta SI es relevante en relacion a la pregunta\n",
    "                \"content\": \"El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd2a0a",
   "metadata": {},
   "source": [
    "## 10. Nodo: Rescribir la pregunta si no es clara\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3 minutos**\n",
    "\n",
    "### Flujo iterativo:\n",
    "Si los docs no son relevantes:\n",
    "1. Reescribir pregunta (mejorar redacci√≥n)\n",
    "2. Volver a generar query\n",
    "3. Buscar de nuevo\n",
    "4. Evaluar de nuevo\n",
    "\n",
    "üí° **Concepto**: Query rewriting ‚Üí b√∫squeda mejorada ‚Üí mejor contexto\n",
    "\n",
    "‚ö†Ô∏è **Nota**: En un grafo real habr√≠a l√≠mite de iteraciones para evitar loops infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d70c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Analiza detenidamente la siguiente pregunta e intenta comprender la intenci√≥n o el significado profundo que transmite.\\n\"\n",
    "    \"Pregunta original:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Ahora, reescribe la pregunta para que sea m√°s clara, precisa y f√°cil de entender:\"\n",
    ")\n",
    "def rescribir_question(state: MessagesState):\n",
    "    \"\"\"Nodo 3: Reescribe la pregunta del usuario para mejorarla.\"\"\"\n",
    "    print(\"\\n‚úèÔ∏è  Reescribiendo pregunta para mejorar b√∫squeda...\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # <FILL_IN> Extrae la pregunta original del usuario\n",
    "    # Busca el √∫ltimo mensaje con role=\"user\" en state[\"messages\"]\n",
    "    question = <FILL_IN>\n",
    "    \n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    print(f\"üìù Pregunta reescrita: {response.content[:100]}...\")\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f8caf-8cb0-435d-ba97-a3680aa8d4f1",
   "metadata": {},
   "source": [
    "### 10.1 Probamos la funci√≥n de rescribir la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e4567-5411-407b-858e-30fa7be4bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQue es batch?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es batch?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Son las 10 de la ma√±ana\", \"tool_call_id\": \"1\"},#respuesta absurda --> Rescribe la pregunta\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rescribir_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd48b18",
   "metadata": {},
   "source": [
    "## 11. Nodo: Generar la respuesta final\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3 minutos**\n",
    "\n",
    "### √öltima etapa del RAG:\n",
    "1. Tienes documentos relevantes (ya evaluados)\n",
    "2. Tienes la pregunta del usuario\n",
    "3. Formato de prompt: pregunta + contexto\n",
    "4. Modelo genera respuesta basada en docs\n",
    "\n",
    "üí° **Prompt engineering**: El GENERATE_PROMPT es crucial\n",
    "- Limita respuesta a 3 frases (concisi√≥n)\n",
    "- Pide admitir ignorancia (\"si no sabes, di que no sabes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f87ea0-0b0f-43d4-8e82-cb9a24138189",
   "metadata": {},
   "source": [
    "### 11.1 Construimos el nodo generate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_respuesta(state: MessagesState):\n",
    "    \"\"\"Nodo 4: Genera la respuesta final basada en el contexto relevante.\"\"\"\n",
    "    print(\"\\nü§ñ Generando respuesta final...\")\n",
    "    \n",
    "    # <FILL_IN> Extrae la pregunta del usuario\n",
    "    question = <FILL_IN>\n",
    "    \n",
    "    # <FILL_IN> Extrae el contexto (documento relevante)\n",
    "    # Busca el √∫ltimo ToolMessage con el contenido recuperado\n",
    "    context = <FILL_IN>\n",
    "    #Hay que formatear el prompt\n",
    "    prompt = GENERATE_PROMPT.<FILL_IN>(question=question, context=context)\n",
    "    #Hay que invocar al modelo con la pregunta ey el contexto\n",
    "    response = response_model.<FILL_IN>([{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Respuesta generada: {response.content[:150]}...\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef50f2f6-1d21-4044-a3c5-5ad3197b3c14",
   "metadata": {},
   "source": [
    "### 11.2 Comprobamos el metodo de generar respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d49a2-0199-4a01-9e66-e69db41326b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQu√© significa el t√©rmino 'batch deployment en databricks' y en qu√© contextos se utiliza?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQu√© significa el t√©rmino 'batch deployment en databriccks' y en qu√© contextos se utiliza?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = genera_respuesta(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eca855",
   "metadata": {},
   "source": [
    "## 12. Construir el grafo (workflow agentico)\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-7 minutos**\n",
    "\n",
    "### Estructura del grafo:\n",
    "```\n",
    "START ‚Üí genera_query_o_responde\n",
    "         ‚Üì\n",
    "    ¬øUsa tool?\n",
    "    /        \\\n",
    "  si         no ‚Üí END\n",
    "  ‚Üì\n",
    "retrieve (ejecuta tool)\n",
    "  ‚Üì\n",
    "grade_documents (eval√∫a relevancia)\n",
    "  /          \\\n",
    "si           no\n",
    "‚Üì            ‚Üì\n",
    "genera_respuesta  rescribir_question\n",
    "‚Üì              ‚Üì\n",
    "END ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Componentes:\n",
    "- **Nodos**: Funciones que ejecutan l√≥gica\n",
    "- **Aristas**: Conexiones entre nodos\n",
    "- **Aristas condicionales**: Decisiones basadas en salida\n",
    "\n",
    "üí° **Pista**: Los nombres de nodos deben coincidir en todo el c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdf910-459c-4a6e-a232-35985a89ef4f",
   "metadata": {},
   "source": [
    "### 12.1 Importamos los elementos necesarios para construir el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd51be5-be05-46d1-ac12-be6e47d387c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1520b93-32cf-4433-8bcd-4829463a9e28",
   "metadata": {},
   "source": [
    "#### 12.2. Ensamblamos el workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a7bd3-4e17-441d-b21b-c4b00d0d2536",
   "metadata": {},
   "source": [
    "#### 12.2.1 A√±adimos los nodos.\n",
    "![Nodos del Grafo](./images/grafo_nodos.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèóÔ∏è  Construyendo el grafo agentico...\\n\")\n",
    "\n",
    "# <FILL_IN> Crea el StateGraph con el estado que usamos (MessagesState)\n",
    "workflow = StateGraph(<FILL_IN>)\n",
    "\n",
    "# <FILL_IN> A√±ade los nodos (nombre, funci√≥n)\n",
    "workflow.add_node(<FILL_IN>, genera_query_o_responde)  # Nodo 1\n",
    "workflow.add_node(\"retrieve\", ToolNode([<FILL_IN>]))  # Ejecutor de tools\n",
    "workflow.add_node(<FILL_IN>, rescribir_question)  # Nodo 3\n",
    "workflow.add_node(<FILL_IN>, genera_respuesta)  # Nodo 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437da262-684d-4946-a3e0-0b5fe2697b36",
   "metadata": {},
   "source": [
    "#### 12.2.2 A√±adimos las aristas.\n",
    "![Aristas del Grafo](./images/grafo_aristas.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecaa9c-aa10-4e47-9bb8-65b1262aa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Arista inicial: START ‚Üí genera_query_o_responde\n",
    "workflow.add_edge(<FILL_IN>, \"<FILL_IN>\")  # START ‚Üí nombreDelPrimerNodo\n",
    "\n",
    "# <FILL_IN> Arista condicional: Si usa herramienta ‚Üí retrieve, sino ‚Üí END\n",
    "workflow.add_conditional_edges(\n",
    "    \"genera_query_o_responde\",\n",
    "    tools_condition,  # Funci√≥n que decide basada en tool_use\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# <FILL_IN> Arista condicional desde retrieve: grade_documents decide el siguiente\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    <FILL_IN>,  # Funci√≥n: grade_documents\n",
    ")\n",
    "\n",
    "# <FILL_IN> Aristas simples\n",
    "workflow.add_edge(\"genera_respuesta\", <FILL_IN>)  # genera_respuesta ‚Üí END\n",
    "workflow.add_edge(\"rescribir_question\", \"genera_query_o_responde\")  # loop\n",
    "\n",
    "print(\"‚úÖ Grafo construido correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0c0cf",
   "metadata": {},
   "source": [
    "## 13. Compilar el grafo\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### ¬øQu√© significa compilar?\n",
    "Convertir la estructura de nodos/aristas en un ejecutable\n",
    "- Valida conexiones\n",
    "- Prepara para ejecuci√≥n\n",
    "\n",
    "üí° **Si hay error aqu√≠**: Revisa nombres de nodos, aristas, y tipos de estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Compila el grafo con workflow.compile()\n",
    "graph = <FILL_IN>.compile()\n",
    "print(\"‚úÖ Grafo compilado y listo para ejecutar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab393e7",
   "metadata": {},
   "source": [
    "## 14. Visualizar el grafo\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### Diagrama visual:\n",
    "- Ayuda a entender el flujo\n",
    "- Facilita debugging\n",
    "- Muestra nodos, aristas, condicionales\n",
    "\n",
    "üí° **Tip**: Si no se visualiza, puede ser problema de dependencias (graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"\\nüìä Visualizando el grafo agentico...\\n\")\n",
    "\n",
    "# <FILL_IN> Visualiza el grafo con graph.get_graph().draw_mermaid_png()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fc041",
   "metadata": {},
   "source": [
    "## 15. Ejecutar el grafo - Modo DEBUG\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos** (depende de pregunta)\n",
    "\n",
    "### Este modo:\n",
    "- Muestra cada nodo por el que pasa\n",
    "- Imprime todos los mensajes intermedios\n",
    "- √ötil para debugging y entender el flujo\n",
    "\n",
    "üéØ **Objetivo**: Ver exactamente c√≥mo se ejecuta el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3be477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"\\nüöÄ Ejecutando el grafo en modo DEBUG...\\n\")\n",
    "\n",
    "# <FILL_IN> Define tu pregunta (sobre tus PDFs)\n",
    "pregunta = \"<FILL_IN>\"\n",
    "print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejecuta el grafo paso a paso\n",
    "for chunk in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]}):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"\\nüìò Nodo ejecutado: {node}\")\n",
    "        print(\"-\" * 60)\n",
    "        messages = update.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_msg = messages[-1]\n",
    "            try:\n",
    "                if hasattr(last_msg, \"content\"):\n",
    "                    print(\"üìù Contenido:\")\n",
    "                    print(last_msg.content[:300] if len(last_msg.content) > 300 else last_msg.content)\n",
    "                elif hasattr(last_msg, \"tool_calls\"):\n",
    "                    print(f\"üîß Tool calls: {[tc.name for tc in last_msg.tool_calls]}\")\n",
    "                else:\n",
    "                    pprint(last_msg)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Ejecuci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32babf2",
   "metadata": {},
   "source": [
    "## 16. Ejecutar el grafo - Modo PRODUCCI√ìN\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos**\n",
    "\n",
    "### Este modo:\n",
    "- Solo muestra la respuesta final\n",
    "- Modo limpio (sin debug)\n",
    "- Usa formateo Markdown para mejor visualizaci√≥n\n",
    "\n",
    "üéØ **Objetivo**: Resultado final en forma de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f556ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"\\nüöÄ Ejecutando el grafo en modo PRODUCCI√ìN...\\n\")\n",
    "\n",
    "# <FILL_IN> Define tu pregunta (sobre tus PDFs)\n",
    "pregunta = \"<FILL_IN>\"\n",
    "print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejecuta el grafo y muestra solo la respuesta final\n",
    "resultado = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]})\n",
    "\n",
    "print(\"\\nüìù RESPUESTA FINAL:\\n\")\n",
    "display(Markdown(resultado[\"messages\"][-1].content))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Consulta completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428bdfb",
   "metadata": {},
   "source": [
    "## 17. Resumen y Conceptos Clave\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-7 minutos** (discusi√≥n/preguntas)\n",
    "\n",
    "### Flujo RAG Agentico completo:\n",
    "1. **Cargar** (secs 1-4): Documentos ‚Üí PDFs\n",
    "2. **Chunking** (sec 5): Documentos ‚Üí Chunks (piezas manejables)\n",
    "3. **Embeddings** (sec 6): Chunks ‚Üí Vectores (n√∫meros)\n",
    "4. **Nodo 1** (sec 8): ¬øBuscar o responder?\n",
    "   - Si buscar ‚Üí Nodo 2 (retrieve)\n",
    "   - Si responder ‚Üí END\n",
    "5. **Nodo 2** (sec 6-7): Buscar documentos similares\n",
    "6. **Nodo Evaluador** (sec 9): ¬øSon relevantes?\n",
    "   - Si relevantes ‚Üí Nodo 4 (responder)\n",
    "   - Si no ‚Üí Nodo 3 (reescribir pregunta)\n",
    "7. **Nodo 3** (sec 10): Mejorar pregunta ‚Üí Volver a Nodo 1\n",
    "8. **Nodo 4** (sec 11): Generar respuesta final ‚Üí END\n",
    "\n",
    "### Conceptos clave aprendidos:\n",
    "- **Embeddings**: Representaci√≥n num√©rica del texto\n",
    "- **Vector Store**: Base de datos de b√∫squeda sem√°ntica\n",
    "- **Agentic Loop**: Ciclo de decisi√≥n autom√°tico\n",
    "- **Retrieval Augmented Generation (RAG)**: Mejorar respuestas con contexto\n",
    "- **Graph State**: M√°quina de estados con LangGraph\n",
    "- **Tool Use**: Modelos pueden llamar herramientas autom√°ticamente\n",
    "\n",
    "### Tips para mejorar:\n",
    "1. **Tuning de chunks**: Experimenta con CHUNK_SIZE y CHUNK_OVERLAP\n",
    "2. **Mejor prompt engineering**: Mejorar GRADE_PROMPT y GENERATE_PROMPT\n",
    "3. **Multiple retrievers**: Combinar BM25 + Sem√°ntico\n",
    "4. **Caching**: Guardar embeddings calculados\n",
    "5. **Evaluaci√≥n**: Metrics como precision@k, NDCG\n",
    "\n",
    "### Siguientes pasos:\n",
    "- üîß Personaliza los prompts para tu caso de uso\n",
    "- üìä Mide la calidad de respuestas (F1-score, BLEU, etc)\n",
    "- üöÄ Despliega en producci√≥n con FastAPI/Streamlit\n",
    "- üìà Monitorea performance en tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d08c5",
   "metadata": {},
   "source": [
    "### Im√°genes originales de referencia\n",
    "\n",
    "#### Grafo - Nodos\n",
    "\n",
    "![Nodos del Grafo](./images/grafo_nodos.jpg)\n",
    "\n",
    "#### Grafo - Aristas y Flujo\n",
    "\n",
    "![Aristas del Grafo](./images/grafo_aristas.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829057f",
   "metadata": {},
   "source": [
    "## 18. Troubleshooting y Preguntas Frecuentes\n",
    "\n",
    "### ‚ùå Errores comunes y soluciones:\n",
    "\n",
    "#### 1. \"GOOGLE_API_KEY not found\"\n",
    "- **Soluci√≥n**: \n",
    "  - Crea archivo `.env` en la carpeta con: `GOOGLE_API_KEY=tu_clave_aqui`\n",
    "  - O exporta en terminal: `$env:GOOGLE_API_KEY='tu_clave'`\n",
    "\n",
    "#### 2. \"No documents loaded\"\n",
    "- **Soluci√≥n**:\n",
    "  - Verifica que `./docs` existe y tiene PDFs\n",
    "  - Prueba: `ls ./docs` o `Get-ChildItem ./docs`\n",
    "  - PDFs deben ser v√°lidos (no corruptos)\n",
    "\n",
    "#### 3. \"Retriever returns empty results\"\n",
    "- **Soluci√≥n**:\n",
    "  - Chunks demasiado grandes: baja CHUNK_SIZE\n",
    "  - Consulta muy diferente: usa similar query a tu contenido\n",
    "  - Vector store vac√≠o: regenera con `Chroma.from_documents(...)`\n",
    "\n",
    "#### 4. \"Tool not used by model\"\n",
    "- **Soluci√≥n**:\n",
    "  - Descripci√≥n de tool no clara: mejora `description`\n",
    "  - Pregunta no requiere b√∫squeda: haz pregunta sobre PDFs\n",
    "  - Modelo no reconoce tool: verifica `bind_tools([tool_name])`\n",
    "\n",
    "#### 5. \"Model rejects structured output\"\n",
    "- **Soluci√≥n**:\n",
    "  - Usa modelo m√°s reciente: \"gemini-2.5-pro\"\n",
    "  - Simplifica GradeDocuments (menos campos)\n",
    "  - Usa `.with_structured_output()` correctamente\n",
    "\n",
    "### üí° Tips de debugging:\n",
    "\n",
    "```python\n",
    "# Ver estructura de state\n",
    "print(\"State keys:\", list(state.keys()))\n",
    "print(\"Messages:\", [m.get(\"role\") for m in state[\"messages\"]])\n",
    "\n",
    "# Debug embeddings\n",
    "test_embedding = embeddings.embed_query(\"test\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "\n",
    "# Ver chunks generados\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i}: {len(chunk.page_content)} chars\")\n",
    "```\n",
    "\n",
    "### üìö Recursos √∫tiles:\n",
    "\n",
    "- [LangChain Docs](https://python.langchain.com/)\n",
    "- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- [Google Gemini API](https://ai.google.dev/)\n",
    "- [Chroma Docs](https://docs.trychroma.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
