{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3121fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q langchain>=0.2.5 langchain-community>=0.2.0 langchain-text-splitters>=0.2.0 langchain-google-genai>=0.0.10 chromadb>=0.5.0 tiktoken>=0.7.0 pypdf>=4 python-dotenv>=1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbae76",
   "metadata": {},
   "source": [
    "## 2. Importamos las librer√≠as necesarias\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### Desglose de importaciones:\n",
    "- **PyPDFLoader**: Lee PDFs\n",
    "- **RecursiveCharacterTextSplitter**: Divide el texto en chunks (trozos)\n",
    "- **Chroma**: Vector database para almacenar embeddings\n",
    "- **GoogleGenerativeAIEmbeddings**: Convierte texto en vectores\n",
    "- **ChatGoogleGenerativeAI**: Modelo de lenguaje Gemini\n",
    "- **tqdm**: Barra de progreso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c154e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (GOOGLE_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat con Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e85d52",
   "metadata": {},
   "source": [
    "## 3. Configuramos el entorno\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3-5 minutos**\n",
    "\n",
    "### Conceptos clave:\n",
    "- **CHUNK_SIZE**: Tama√±o de cada fragmento de texto (ej: 1000 caracteres)\n",
    "- **CHUNK_OVERLAP**: Solapamiento entre chunks (ej: 200 caracteres) para mantener contexto\n",
    "- **TOP_K**: N√∫mero de documentos m√°s relevantes a recuperar\n",
    "- **Modelos**: Necesitas elegir embeddings y modelo de chat\n",
    "\n",
    "üí° **Tip**: Valores t√≠picos:\n",
    "- CHUNK_SIZE: 500-2000\n",
    "- CHUNK_OVERLAP: 100-400 (~20% del CHUNK_SIZE)\n",
    "- Embedding: \"models/text-embedding-004\"\n",
    "- Chat: \"gemini-2.5-flash\" o \"gemini-1.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d5fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GOOGLE_API_KEY detectada\n",
      "üìÅ Carpeta PDFs: C:\\Trainings\\GenIA_trainings\\Training\\docs\n",
      "üóÇÔ∏è  Persistencia Chroma: C:\\Trainings\\GenIA_trainings\\Training\\chroma_pdfs_gemini\n",
      "üîß Configuraci√≥n: CHUNK_SIZE=800, OVERLAP=120, TOP_K=4\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs_gemini\")  # Donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Valores completados para prueba\n",
    "CHUNK_SIZE = 800  # Tama√±o de cada chunk (trozo) de texto. Rango: 500-2000\n",
    "CHUNK_OVERLAP = 120  # Solapamiento entre chunks. Recomendado: ~20% del CHUNK_SIZE\n",
    "TOP_K = 4  # N√∫mero de documentos recuperados (mant√©n este valor)\n",
    "\n",
    "# Modelos de Gemini\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"  # Modelo de embeddings\n",
    "CHAT_MODEL = \"gemini-2.5-flash\"  # Modelo de chat\n",
    "\n",
    "# Verificar clave - SIN ESTO, NO FUNCIONA\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"‚ùå Falta GOOGLE_API_KEY en variables de entorno o .env\"\n",
    "print(f\"‚úÖ GOOGLE_API_KEY detectada\")\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è  Persistencia Chroma: {PERSIST_DIR.resolve()}\")\n",
    "print(f\"üîß Configuraci√≥n: CHUNK_SIZE={CHUNK_SIZE}, OVERLAP={CHUNK_OVERLAP}, TOP_K={TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eb980f",
   "metadata": {},
   "source": [
    "## 4. Cargamos los PDFs\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos** (seg√∫n cantidad de PDFs)\n",
    "\n",
    "### ¬øQu√© ocurre aqu√≠?\n",
    "1. Lee todos los PDFs de la carpeta `./docs`\n",
    "2. A√±ade metadatos (origen del documento) a cada p√°gina\n",
    "3. Retorna lista de documentos cargados\n",
    "\n",
    "üéØ **Objetivo**: Tener una lista de documentos listos para procesar\n",
    "\n",
    "üí° **Tip de debugging**: Si no ves docs, verifica que:\n",
    "- Los PDFs est√°n en `./docs`\n",
    "- Los PDFs no est√°n corruptos\n",
    "- Tienes permisos de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff80334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cargando PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Documentos (p√°ginas) cargados: 221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    \"\"\"Carga todos los PDFs de una carpeta de forma recursiva.\"\"\"\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(pdf_paths, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88029bce",
   "metadata": {},
   "source": [
    "## 5. Chunking.\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1-2 minutos**\n",
    "\n",
    "### ¬øPor qu√© hacer chunking?\n",
    "- Los modelos tienen l√≠mite de tokens (palabras)\n",
    "- Dividir en trozos permite recuperar partes relevantes\n",
    "- El solapamiento preserva contexto entre chunks\n",
    "\n",
    "### Par√°metros de recursi√≥n:\n",
    "- `separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]` ‚Üí Intenta respetar p√°rrafos, luego l√≠neas, luego palabras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99d8740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è  Chunks generados: 501\n",
      "üìà Ratio de expansi√≥n: 501 chunks de 221 documentos\n",
      "üìù Ejemplo de primer chunk (primeros 200 caracteres):\n",
      "Lava 600 - Primary\n",
      "#FF3621\n",
      "RGB (255 ,54, 33)\n",
      "C0, M91, Y93, K0\n",
      "Navy 800 - Primary\n",
      "#1B3139\n",
      "RGB (27, 49, 57)\n",
      "C86, M65, Y57, K56\n",
      "Maroon 600\n",
      "#98102A\n",
      "RGB (152, 16, 42)\n",
      "C26, M100, Y84, K24\n",
      "Yellow 600\n",
      "#FFAB00...\n"
     ]
    }
   ],
   "source": [
    "# Crea el splitter con los par√°metros configurados\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"‚úÇÔ∏è  Chunks generados: {len(chunks)}\")\n",
    "print(f\"üìà Ratio de expansi√≥n: {len(chunks)} chunks de {len(raw_docs)} documentos\")\n",
    "if chunks:\n",
    "    print(f\"üìù Ejemplo de primer chunk (primeros 200 caracteres):\\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d67fb",
   "metadata": {},
   "source": [
    "## 6. Crear Embeddings y Vector Store\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-15 minutos** (seg√∫n cantidad de chunks)\n",
    "\n",
    "### Conceptos:\n",
    "- **Embeddings**: Convertir texto a n√∫meros (vectores)\n",
    "  - El modelo de embeddings crea una representaci√≥n num√©rica\n",
    "  - Textos similares tienen vectores cercanos\n",
    "- **Vector Store (Chroma)**: Base de datos de vectores\n",
    "  - Almacena chunks + sus embeddings\n",
    "  - Permite b√∫squeda sem√°ntica r√°pida\n",
    "- **Retriever**: Interfaz para recuperar documentos similares\n",
    "\n",
    "üí° **Pista**: Este paso es intensivo. Es normal esperar.\n",
    "\n",
    "üéØ **Prueba de checkpoint**: El vector store debe persistirse en `./chroma_pdfs_gemini`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac95c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings inicializados\n",
      "‚úÖ Vector store creado con 501 chunks\n",
      "‚úÖ Retriever creado y listo para b√∫squedas\n"
     ]
    }
   ],
   "source": [
    "# Crea los embeddings con el modelo configurado\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "print(\"‚úÖ Embeddings inicializados\")\n",
    "\n",
    "# Crea el vector store con Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "print(f\"‚úÖ Vector store creado con {len(chunks)} chunks\")\n",
    "\n",
    "# Crea el retriever a partir del vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "print(\"‚úÖ Retriever creado y listo para b√∫squedas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e628d6",
   "metadata": {},
   "source": [
    "## 7. Creamos la Tool (Retriever) para el agente\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 2 minutos**\n",
    "\n",
    "### ¬øQu√© es una Tool?\n",
    "Una herramienta que el agente puede usar durante su ejecuci√≥n:\n",
    "- **Nombre**: Identificador √∫nico\n",
    "- **Descripci√≥n**: Qu√© hace (el modelo la lee para decidir si usarla)\n",
    "- **Funci√≥n**: El retriever que implementa la b√∫squeda\n",
    "\n",
    "üí° **Pista**: Una buena descripci√≥n ayuda al modelo a saber cu√°ndo usar esta herramienta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3e1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever tool 'pdf_search' creada\n",
      "   Descripci√≥n: Busca informaci√≥n en los PDFs cargados. √ötil para encontrar respuestas sobre documentos PDF.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Crea la retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"pdf_search\",  # nombre de la tool\n",
    "    \"Busca informaci√≥n en los PDFs cargados. √ötil para encontrar respuestas sobre documentos PDF.\",  # descripci√≥n\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Retriever tool '{retriever_tool.name}' creada\")\n",
    "print(f\"   Descripci√≥n: {retriever_tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d614",
   "metadata": {},
   "source": [
    "### 7.1 Test de la tool\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 2 minutos**\n",
    "\n",
    "**Objetivo**: Verificar que la tool funciona correctamente\n",
    "- Ejecuta una b√∫squeda real contra tu vector store\n",
    "- Observa qu√© documentos se recuperan\n",
    "- Valida que son relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3536bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Documentos recuperados:\n",
      "------------------------------------------------------------\n",
      "Lava 500\n",
      "#FF5F46\n",
      "RGB (255, 95, 70)\n",
      "C0, M78, Y79, K0\n",
      "Navy 900\n",
      "#0B2026\n",
      "RGB (11, 32, 38)\n",
      "C86, M67, Y61,  K71\n",
      "¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \n",
      "Iceberg logo are trademarks of the Apache Software Foundation.\n",
      "Batch Deployment\n",
      "‚óè Batch processing generates predictions on a regular schedule and \n",
      "writes the results out to persistent storage to be consumed downstream \n",
      "(i.e. ad-hoc BI).\n",
      "‚óè Batch deployment is the simplest deployment strategy.\n",
      "‚óè Ideal for cases when: \n",
      "‚óã Immediate predictions is not necessary\n",
      "‚óã Predictions can be made in batch fashion\n",
      "‚óã Number/volume of (new) records/observations to predict is large\n",
      "‚óã Pace at which input/records change or is received is > 30 mins\n",
      "\n",
      "Lava 500\n",
      "#FF5F46\n",
      "RGB (255, 95, 70)\n",
      "C0, M78, Y79, K0\n",
      "Navy 900\n",
      "#0B2026\n",
      "RGB (11, 32, 38)\n",
      "C86, M67, Y61,  K71\n",
      "¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \n",
      "Iceberg logo are trademarks of the Apache Software Foundation.\n",
      "Batch Deployment\n",
      "‚óè Batch processing generates predictions on a regular schedule and \n",
      "writes the results out to persistent storage to be consumed downstream \n",
      "(i.e. ad-hoc BI).\n",
      "‚óè Batch deployment is the simplest deployment strategy.\n",
      "‚óè Ideal for cases when: \n",
      "‚óã Immediate predictions is not necessary\n",
      "‚óã Predictions can be made in batch fashion\n",
      "‚óã Number/volume of (new) records/observations to predict is large\n",
      "‚óã Pace at which input/records change or is received is > 30 mins\n",
      "\n",
      "Lava 500\n",
      "#FF5F46\n",
      "RGB (255, 95, 70)\n",
      "C0, M78, Y79, K0\n",
      "Navy 900\n",
      "#0B2026\n",
      "RGB (11, 32, 38)\n",
      "C86, M67, Y61,  K71\n",
      "¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \n",
      "Iceberg logo are trademarks of the Apache Software Foundation.\n",
      "Batch Deployment\n",
      "‚óè Batch processing generates predictions on a regular schedule and \n",
      "writes the results out to persistent storage to be consumed downstream \n",
      "(i.e. ad-hoc BI).\n",
      "‚óè Batch deployment is the simplest deployment strategy.\n",
      "‚óè Ideal for cases when: \n",
      "‚óã Immediate predictions is not necessary\n",
      "‚óã Predictions can be made in batch fashion\n",
      "‚óã Number/volume of (new) records/observations to predict is large\n",
      "‚óã Pace at which input/records change or is received is > 30 mins\n",
      "\n",
      "Lava 500\n",
      "#FF5F46\n",
      "RGB (255, 95, 70)\n",
      "C0, M78, Y79, K0\n",
      "Navy 900\n",
      "#0B2026\n",
      "RGB (11, 32, 38)\n",
      "C86, M67, Y61,  K71\n",
      "¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \n",
      "Iceberg logo are trademarks of the Apache Software Foundation.\n",
      "Batch Deployment\n",
      "‚óè Batch processing generates predictions on a regular schedule and \n",
      "writes the results out to persistent storage to be consumed downstream \n",
      "(i.e. ad-hoc BI).\n",
      "‚óè Batch deployment is the simplest deployment strategy.\n",
      "‚óè Ideal for cases when: \n",
      "‚óã Immediate predictions is not necessary\n",
      "‚óã Predictions can be made in batch fashion\n",
      "‚óã Number/volume of (new) records/observations to predict is large\n",
      "‚óã Pace at which input/records change or is received is > 30 mins\n"
     ]
    }
   ],
   "source": [
    "# Prueba la tool con una pregunta sobre tus PDFs\n",
    "resultado=retriever_tool.invoke({\"query\": \"Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue es un deployment tipo batch?\"})\n",
    "print(\"\\nüìÑ Documentos recuperados:\")\n",
    "print(\"-\" * 60)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d950c",
   "metadata": {},
   "source": [
    "## 8. Nodo: Genera query o responde directamente\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5 minutos**\n",
    "\n",
    "### Flujo del agente (agentico loop):\n",
    "1. **Este nodo**: Recibe la pregunta del usuario\n",
    "2. Decide: ¬øNecesito buscar docs o puedo responder directamente?\n",
    "3. Si usa la tool ‚Üí pasa a retriever\n",
    "4. Si responde ‚Üí termina\n",
    "\n",
    "### Conceptos clave:\n",
    "- **bind_tools**: Conecta herramientas al modelo\n",
    "- **MessagesState**: Estado que mantiene el historial de mensajes\n",
    "- **ToolUse**: Cuando el modelo elige usar una herramienta\n",
    "\n",
    "üí° **Pista**: Es el primer nodo del grafo agentico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7fef1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo de chat gemini-2.5-flash configurado\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# Instancia el modelo de chat con el modelo configurado\n",
    "response_model = ChatGoogleGenerativeAI(model=CHAT_MODEL, temperature=0)\n",
    "print(f\"‚úÖ Modelo de chat {CHAT_MODEL} configurado\")\n",
    "\n",
    "def genera_query_o_responde(state: MessagesState):\n",
    "    \"\"\"Nodo 1: Decide si recuperar informaci√≥n o responder directamente.\"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283a104",
   "metadata": {},
   "source": [
    "### 8.1 Test: Pregunta que no necesita b√∫squeda\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "**Esperado**: El modelo responde directamente sin usar la tool\n",
    "- Pregunta gen√©rica (ej: \"¬øCuanto vale una cortina?\")\n",
    "- Observa que NO hay tool_use en la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3858e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test 1: Pregunta sin necesidad de b√∫squeda\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'type': 'text', 'text': 'No tengo informaci√≥n sobre el precio de las cortinas. Mi funci√≥n es buscar informaci√≥n en los PDFs que me has proporcionado. Si la informaci√≥n que buscas est√° en alguno de ellos, por favor, h√°zmelo saber.', 'extras': {'signature': 'CogFAdHtim+k0VBxDVhKGUJv2Lg5d3qVEXgXND3WXzX3MjtnZyNt5Wm830vUSX4LACvh0bYzd3i4hLxQbGE8jDVS9o4FCgRBOOPOaPLd7hkcdT1Jc2x1ZIaKOs7P7+tQS4ai3fk0nh6xlAXVWvpuhz7TxDsU5zCUBlfaMx5b/IJuPhGK8S277TI8GVtpLv/fTNlNAcgXvUX5gqchnALIUeZMWuWA0octMY2b4QIDheIIvABP7LUshzD9WE8J+rhvBIwUJSY4tmSmlJ/27BtNyXztYy0Iz82bmLNaIPtlA1OoPre54CFVHrylJtv6tphvUgg0IXFUVRMTl1ndRWejG/GIas0d5ZyYE85J4xDa9fVlOePgTiHxmotD3aBE6a3N8XksBtqO4AmyASGz4Vj/P4E31pvbn0PYt9u+b6XX4gA/HlPr0PyaR+pYwK8ZPQCuqZJQIfkSX1VWkAPj4T1O15zvxY+3LooB/Va/gqEokyUdLlKm/TYbEOf8f37L0S46SIDQM+Z0ezl34mqxQD/dt7d1aTztDR8FKXnTtuXeLshGbNoM7T3n5iFcO2Q8+xAO0ky1qhRN/snWvOy0w1ctkxO+qQqQqXng+4mdENP592oV/Ci6LrH6yx/cxX6kSYnNssWVAjDbe7L2kd3q7APUKd62+eGWhh0fMFJTNrjrEvTa7krWR64u2qL433Y0ZofXUfNoHKKqAigUXx2sqmweXf5CAwHSwoTY9GcZswRzTeSOFiLSGtCnCRC8Zz51zLOSgr6Oy4EoFyBHVX0A5E6yu9sTZxtYSipvMeaNJ3Io3SCMAB4hp3GsPS3PJw/ltS+WedO4xekwi7HFJIEFUyL1omZey3blZbbb5rvi'}}]\n",
      "\n",
      "‚úÖ Observa que NO hay 'tool_use' en la respuesta\n"
     ]
    }
   ],
   "source": [
    "# Prueba con una pregunta general\n",
    "input_test = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCuanto vale una cortina?\"}]}\n",
    "print(\"\\nüß™ Test 1: Pregunta sin necesidad de b√∫squeda\\n\")\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()\n",
    "print(\"\\n‚úÖ Observa que NO hay 'tool_use' en la respuesta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14658619",
   "metadata": {},
   "source": [
    "### 8.2 Test: Pregunta que requiere b√∫squeda sem√°ntica\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "**Esperado**: El modelo usa la tool para buscar documentos\n",
    "- Pregunta sobre contenido de tus PDFs\n",
    "- Observa que S√ç hay tool_use con el nombre de tu tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bbe6e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test 2: Pregunta que REQUIERE b√∫squeda\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  pdf_search (c5629370-7ffc-423a-9935-5ccace5fee35)\n",
      " Call ID: c5629370-7ffc-423a-9935-5ccace5fee35\n",
      "  Args:\n",
      "    query: ¬øQue es un deployment tipo batch?\n",
      "\n",
      "‚úÖ Observa que S√ç hay 'tool_use' (tu tool debe estar en tool_calls)\n"
     ]
    }
   ],
   "source": [
    "# Prueba con una pregunta que REQUIERE b√∫squeda\n",
    "input_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue es un deployment tipo batch?\",\n",
    "            #\"content\": \"¬øQue es un  batch?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(\"\\nüß™ Test 2: Pregunta que REQUIERE b√∫squeda\\n\")\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()\n",
    "print(\"\\n‚úÖ Observa que S√ç hay 'tool_use' (tu tool debe estar en tool_calls)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45a147",
   "metadata": {},
   "source": [
    "## 9. Nodo: Evaluar relevancia de documentos\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5 minutos**\n",
    "\n",
    "### ¬øPor qu√© evaluar relevancia?\n",
    "- No siempre la b√∫squeda sem√°ntica recupera docs relevantes\n",
    "- Control de calidad: rechazar docs no pertinentes\n",
    "- Bifurcaci√≥n: docs relevantes ‚Üí responder, docs irrelevantes ‚Üí reescribir pregunta\n",
    "\n",
    "### GradeDocuments (Pydantic):\n",
    "- Estructura de datos con score binario (\"si\"/\"no\")\n",
    "- Ayuda a parsear la respuesta del modelo\n",
    "\n",
    "üí° **Concepto**: Los modelos pueden ser instructores, pero tambi√©n evaluadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183d40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"Eres un evaluador que determina la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \"\n",
    "    \"Aqu√≠ tienes el documento recuperado: \\n\\n {context} \\n\\n\"\n",
    "    \"Aqu√≠ tienes la pregunta del usuario: {question} \\n\"\n",
    "    \"Si el documento contiene palabra(s) clave o significado sem√°ntico relacionado con la pregunta del usuario, calif√≠calo como relevante. \\n\"\n",
    "    \"Da una puntuaci√≥n binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Califica los documentos utilizando una puntuaci√≥n binaria para comprobar su relevancia\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Puntuaci√≥n : 'si' si es relevante, o 'no' si no lo es\"\n",
    "    )\n",
    "\n",
    "grader_model = ChatGoogleGenerativeAI(model=CHAT_MODEL, temperature=0)\n",
    "\n",
    "def grade_documents(state: MessagesState) -> Literal[\"genera_respuesta\", \"rescribir_question\"]:\n",
    "    \"\"\"Nodo 2: Eval√∫a si los documentos recuperados son relevantes.\n",
    "    \n",
    "    Retorna:\n",
    "    - \"genera_respuesta\" si docs son relevantes\n",
    "    - \"rescribir_question\" si no lo son\n",
    "    \"\"\"\n",
    "    print(\"\\n‚è≥ Evaluando relevancia de documentos...\")\n",
    "    \n",
    "    # Extrae la pregunta\n",
    "    question = state[\"messages\"][0].content\n",
    "    \n",
    "    # Extrae el contexto (√∫ltima respuesta)\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "    print(f\"üìä Score de relevancia: {score}\")\n",
    "    \n",
    "    if score == \"si\":\n",
    "        print(\"‚úÖ Docs relevantes ‚Üí generando respuesta\")\n",
    "        return \"genera_respuesta\"\n",
    "    else:\n",
    "        print(\"‚ùå Docs no relevantes ‚Üí reescribiendo pregunta\")\n",
    "        return \"rescribir_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672a823-9f9f-4127-9a7d-7dbd798919a1",
   "metadata": {},
   "source": [
    "#### 9.1 Comprobamos con una respuesta irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c66f0c5f-0a5e-4829-a229-1d3a25e46903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Evaluando relevancia de documentos...\n",
      "üìä Score de relevancia: no\n",
      "‚ùå Docs no relevantes ‚Üí reescribiendo pregunta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rescribir_question'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simulamos la respuesta de la tool mediante mensajes\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQue es un deployment batch?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es un deployment batch?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Son las 10 de la ma√±ana\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b609-f401-46b0-a18c-1d641701a722",
   "metadata": {},
   "source": [
    "#### 9.2 Comprobar que el documento/respuesta relevante lo clasifica como tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bde7d95-0e72-44df-bc02-1ffc6bfba4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Evaluando relevancia de documentos...\n",
      "üìä Score de relevancia: si\n",
      "‚úÖ Docs relevantes ‚Üí generando respuesta\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'genera_respuesta'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "               \n",
    "                \"content\": \"¬øQue es un deployment batch en el documento?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es un deployment batch en el documento?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks.\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da92347",
   "metadata": {},
   "source": [
    "## 10. Nodo: Rescribir la pregunta si no es lo suficientemente clara\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3 minutos**\n",
    "\n",
    "### Flujo iterativo:\n",
    "Si los docs no son relevantes:\n",
    "1. Reescribir pregunta (mejorar redacci√≥n)\n",
    "2. Volver a generar query\n",
    "3. Buscar de nuevo\n",
    "4. Evaluar de nuevo\n",
    "\n",
    "üí° **Concepto**: Query rewriting ‚Üí b√∫squeda mejorada ‚Üí mejor contexto\n",
    "\n",
    "‚ö†Ô∏è **Nota**: En un grafo real habr√≠a l√≠mite de iteraciones para evitar loops infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5bf727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Analiza detenidamente la siguiente pregunta e intenta comprender la intenci√≥n o el significado profundo que transmite.\\n\"\n",
    "    \"Pregunta original:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Ahora, reescribe la pregunta para que sea m√°s clara, precisa y f√°cil de entender:\"\n",
    ")\n",
    "\n",
    "def rescribir_question(state: MessagesState):\n",
    "    \"\"\"Nodo 3: Reescribe la pregunta del usuario para mejorarla.\"\"\"\n",
    "    print(\"\\n‚úèÔ∏è  Reescribiendo pregunta para mejorar b√∫squeda...\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Extrae la pregunta original\n",
    "    question = messages[0].content\n",
    "    \n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    print(f\"üìù Pregunta reescrita: {response.content[:100]}...\")\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf3cff-b09e-4cc9-bd66-ed74ae32b98c",
   "metadata": {},
   "source": [
    "### 10.1 Probamos la funci√≥n de rescribir la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f60bdec1-d7bc-4f7b-9804-77145dfbc816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úèÔ∏è  Reescribiendo pregunta para mejorar b√∫squeda...\n",
      "üìù Pregunta reescrita: La pregunta original \"¬øQu√© es batch?\" es demasiado ambigua debido a que la palabra \"batch\" tiene m√∫l...\n",
      "La pregunta original \"¬øQu√© es batch?\" es demasiado ambigua debido a que la palabra \"batch\" tiene m√∫ltiples significados dependiendo del contexto (inform√°tica, producci√≥n, qu√≠mica, etc.). La intenci√≥n profunda es, probablemente, obtener una definici√≥n o explicaci√≥n de \"batch\" en un contexto espec√≠fico que el usuario tiene en mente, pero que no ha explicitado.\n",
      "\n",
      "Para hacerla m√°s clara, precisa y f√°cil de entender, necesitamos a√±adir contexto o pedir al usuario que lo especifique.\n",
      "\n",
      "Aqu√≠ tienes varias opciones, dependiendo del contexto m√°s probable o de c√≥mo quieras guiar al usuario:\n",
      "\n",
      "**Opci√≥n 1 (La m√°s completa, pidiendo contexto):**\n",
      "\n",
      "> \"Para poder responder con precisi√≥n, ¬øpodr√≠as especificar el contexto en el que te refieres a 'batch'? Por ejemplo, ¬øest√°s preguntando sobre:\n",
      ">\n",
      "> *   El concepto de **procesamiento por lotes (batch processing)** en inform√°tica?\n",
      "> *   Los **archivos batch (.bat o .cmd)** en sistemas operativos?\n",
      "> *   El significado general de **'un lote' o 'una tanda'** de algo (como en producci√≥n o cocina)?\n",
      "> *   Alg√∫n otro contexto espec√≠fico?\"\n",
      "\n",
      "**Opci√≥n 2 (Asumiendo un contexto inform√°tico, que es muy com√∫n):**\n",
      "\n",
      "> \"¬øPodr√≠as explicar qu√© es el **procesamiento por lotes (batch processing)** en el √°mbito de la inform√°tica o los sistemas?\"\n",
      "\n",
      "**Opci√≥n 3 (M√°s espec√≠fica dentro de inform√°tica):**\n",
      "\n",
      "> \"¬øQu√© es un **archivo batch** (como los archivos .bat o .cmd) y para qu√© se utiliza en sistemas operativos como Windows?\"\n",
      "\n",
      "**Opci√≥n 4 (Si se busca una definici√≥n general):**\n",
      "\n",
      "> \"¬øCu√°l es la definici√≥n general de la palabra 'batch' (lote o tanda) y c√≥mo se aplica en diferentes contextos?\"\n",
      "\n",
      "**La mejor reescritura, que aborda la ambig√ºedad directamente y ofrece las opciones m√°s comunes, ser√≠a la Opci√≥n 1.**\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQue es batch?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQue es batch?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"Son las 10 de la ma√±ana\", \"tool_call_id\": \"1\"},#respuesta absurda\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rescribir_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687784e8",
   "metadata": {},
   "source": [
    "## 11. Nodo: Generar la respuesta final\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 3 minutos**\n",
    "\n",
    "### √öltima etapa del RAG:\n",
    "1. Tienes documentos relevantes (ya evaluados)\n",
    "2. Tienes la pregunta del usuario\n",
    "3. Formato de prompt: pregunta + contexto\n",
    "4. Modelo genera respuesta basada en docs\n",
    "\n",
    "üí° **Prompt engineering**: El GENERATE_PROMPT es crucial\n",
    "- Limita respuesta a 3 frases (concisi√≥n)\n",
    "- Pide admitir ignorancia (\"si no sabes, di que no sabes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8bf987-dc2b-4d7d-8711-c7e27294a94c",
   "metadata": {},
   "source": [
    "### 11.1 Construimos el nodo generate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "372cde6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"Eres un asistente para tareas de preguntas y respuestas. \"\n",
    "    \"Utiliza los siguientes fragmentos de contexto recuperado para responder a la pregunta. \"\n",
    "    \"Si no sabes la respuesta, simplemente indica que no la sabes. \"\n",
    "    \"Utiliza un m√°ximo de tres frases y mant√©n la respuesta concisa.\\n\"\n",
    "    \"Pregunta: {question} \\n\"\n",
    "    \"Contexto: {context}\"\n",
    ")\n",
    "\n",
    "def genera_respuesta(state: MessagesState):\n",
    "    \"\"\"Nodo 4: Genera la respuesta final basada en el contexto relevante.\"\"\"\n",
    "    print(\"\\nü§ñ Generando respuesta final...\")\n",
    "    \n",
    "    # Extrae la pregunta\n",
    "    question = state[\"messages\"][0].content\n",
    "    \n",
    "    # Extrae el contexto (√∫ltimo mensaje)\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Respuesta generada: {response.content[:150]}...\")\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633a07a-fb38-48a9-a871-dde1fa779f32",
   "metadata": {},
   "source": [
    "### 11.2 Comprobamos el metodo de generar respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad199a4-3f0b-4e40-a0bf-8fb6af9f1e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generando respuesta final...\n",
      "\n",
      "‚úÖ Respuesta generada: El contexto proporcionado no ofrece una definici√≥n expl√≠cita del t√©rmino 'batch deployment'. Sin embargo, se refiere a √©l como un m√©todo de despliegue...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "El contexto proporcionado no ofrece una definici√≥n expl√≠cita del t√©rmino 'batch deployment'. Sin embargo, se refiere a √©l como un m√©todo de despliegue de modelos en Databricks. Los objetivos de aprendizaje relacionados incluyen describir sus escenarios de uso, identificar sus ventajas y desventajas, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"¬øQu√© significa el t√©rmino 'batch deployment en databricks' y en qu√© contextos se utiliza?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"¬øQu√© significa el t√©rmino 'batch deployment en databriccks' y en qu√© contextos se utiliza?\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"El contexto proporcionado hace referencia a 'batch deployment' (despliegue por lotes) en el documento, pero no ofrece una definici√≥n expl√≠cita de qu√© es 'batch'. Sin embargo, se menciona que uno de los objetivos de aprendizaje es describir el despliegue en batch y sus escenarios de uso, as√≠ como identificar las ventajas y desventajas de desplegar un modelo mediante procesamiento por lotes, y discutir un flujo de trabajo t√≠pico para este tipo de despliegue en Databricks\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = genera_respuesta(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb265f",
   "metadata": {},
   "source": [
    "## 12. Construir el grafo (workflow agentico)\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-7 minutos**\n",
    "\n",
    "### Estructura del grafo:\n",
    "```\n",
    "START ‚Üí genera_query_o_responde\n",
    "         ‚Üì\n",
    "    ¬øUsa tool?\n",
    "    /        \\\n",
    "  si         no ‚Üí END\n",
    "  ‚Üì\n",
    "retrieve (ejecuta tool)\n",
    "  ‚Üì\n",
    "grade_documents (eval√∫a relevancia)\n",
    "  /          \\\n",
    "si           no\n",
    "‚Üì            ‚Üì\n",
    "genera_respuesta  rescribir_question\n",
    "‚Üì              ‚Üì\n",
    "END ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Componentes:\n",
    "- **Nodos**: Funciones que ejecutan l√≥gica\n",
    "- **Aristas**: Conexiones entre nodos\n",
    "- **Aristas condicionales**: Decisiones basadas en salida\n",
    "\n",
    "üí° **Pista**: Los nombres de nodos deben coincidir en todo el c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552777a7-c964-418f-b3da-323cc6820027",
   "metadata": {},
   "source": [
    "### 12.1 Importamos los elementos necesarios para construir el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a460acb2-459f-40a9-86bd-e7a285ad9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e128ed6-1210-4fe4-a983-14c48fc6ace1",
   "metadata": {},
   "source": [
    "#### 12.2. Ensamblamos el workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2048d04-a9e9-4cca-a25d-ae1adfac25cb",
   "metadata": {},
   "source": [
    "#### 12.2.1 A√±adimos los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8126a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  Construyendo el grafo agentico...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1a666b18980>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüèóÔ∏è  Construyendo el grafo agentico...\\n\")\n",
    "\n",
    "# Crea el StateGraph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# A√±ade los nodos\n",
    "workflow.add_node(\"genera_query_o_responde\", genera_query_o_responde)  # Nodo 1\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))  # Ejecutor de tools\n",
    "workflow.add_node(\"rescribir_question\", rescribir_question)  # Nodo 3\n",
    "workflow.add_node(\"genera_respuesta\", genera_respuesta)  # Nodo 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0c182-63cc-4839-ae4e-921b1f4b0236",
   "metadata": {},
   "source": [
    "#### 12.2.2 A√±adimos las aristas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b42083-0a9b-4777-832d-5ea9268d7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grafo construido correctamente\n"
     ]
    }
   ],
   "source": [
    "# Arista inicial\n",
    "workflow.add_edge(START, \"genera_query_o_responde\")\n",
    "\n",
    "# Arista condicional: Si usa herramienta\n",
    "workflow.add_conditional_edges(\n",
    "    \"genera_query_o_responde\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Arista condicional desde retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    ")\n",
    "\n",
    "# Aristas simples\n",
    "workflow.add_edge(\"genera_respuesta\", END)\n",
    "workflow.add_edge(\"rescribir_question\", \"genera_query_o_responde\")\n",
    "\n",
    "print(\"‚úÖ Grafo construido correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f08c46",
   "metadata": {},
   "source": [
    "## 13. Compilar el grafo\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### ¬øQu√© significa compilar?\n",
    "Convertir la estructura de nodos/aristas en un ejecutable\n",
    "- Valida conexiones\n",
    "- Prepara para ejecuci√≥n\n",
    "\n",
    "üí° **Si hay error aqu√≠**: Revisa nombres de nodos, aristas, y tipos de estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a7895fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grafo compilado y listo para ejecutar\n"
     ]
    }
   ],
   "source": [
    "# Compila el grafo\n",
    "graph = workflow.compile()\n",
    "print(\"‚úÖ Grafo compilado y listo para ejecutar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0c50a",
   "metadata": {},
   "source": [
    "## 14. Visualizar el grafo\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 1 minuto**\n",
    "\n",
    "### Diagrama visual:\n",
    "- Ayuda a entender el flujo\n",
    "- Facilita debugging\n",
    "- Muestra nodos, aristas, condicionales\n",
    "\n",
    "üí° **Tip**: Si no se visualiza, puede ser problema de dependencias (graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9aaa42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Visualizando el grafo agentico...\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"\\nüìä Visualizando el grafo agentico...\\n\")\n",
    "\n",
    "# Visualiza el grafo\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9fe27",
   "metadata": {},
   "source": [
    "## 15. Ejecutar el grafo - Modo DEBUG\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos** (depende de pregunta)\n",
    "\n",
    "### Este modo:\n",
    "- Muestra cada nodo por el que pasa\n",
    "- Imprime todos los mensajes intermedios\n",
    "- √ötil para debugging y entender el flujo\n",
    "\n",
    "üéØ **Objetivo**: Ver exactamente c√≥mo se ejecuta el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb71006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ejecutando el grafo en modo DEBUG...\n",
      "\n",
      "‚ùì Pregunta: Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue es un deployment tipo batch?\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìò Nodo ejecutado: genera_query_o_responde\n",
      "------------------------------------------------------------\n",
      "üìù Contenido:\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚è≥ Evaluando relevancia de documentos...\n",
      "üìä Score de relevancia: si\n",
      "‚úÖ Docs relevantes ‚Üí generando respuesta\n",
      "\n",
      "üìò Nodo ejecutado: retrieve\n",
      "------------------------------------------------------------\n",
      "üìù Contenido:\n",
      "Lava 500\n",
      "#FF5F46\n",
      "RGB (255, 95, 70)\n",
      "C0, M78, Y79, K0\n",
      "Navy 900\n",
      "#0B2026\n",
      "RGB (11, 32, 38)\n",
      "C86, M67, Y61,  K71\n",
      "¬© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache \n",
      "Iceberg logo are trademarks of the Apache Software Foundation.\n",
      "Batch\n",
      "------------------------------------------------------------\n",
      "\n",
      "ü§ñ Generando respuesta final...\n",
      "\n",
      "‚úÖ Respuesta generada: Un deployment tipo batch implica un procesamiento que genera predicciones en un horario regular. Estas predicciones se escriben en almacenamiento pers...\n",
      "\n",
      "üìò Nodo ejecutado: genera_respuesta\n",
      "------------------------------------------------------------\n",
      "üìù Contenido:\n",
      "Un deployment tipo batch implica un procesamiento que genera predicciones en un horario regular. Estas predicciones se escriben en almacenamiento persistente para ser consumidas posteriormente, por ejemplo, en BI ad-hoc. Es la estrategia de despliegue m√°s sencilla y es ideal cuando no se necesitan p\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "‚úÖ Ejecuci√≥n completada\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"\\nüöÄ Ejecutando el grafo en modo DEBUG...\\n\")\n",
    "\n",
    "# Define tu pregunta\n",
    "pregunta = \"Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue es un deployment tipo batch?\"\n",
    "print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejecuta el grafo paso a paso\n",
    "for chunk in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]}):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"\\nüìò Nodo ejecutado: {node}\")\n",
    "        print(\"-\" * 60)\n",
    "        messages = update.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_msg = messages[-1]\n",
    "            try:\n",
    "                if hasattr(last_msg, \"content\"):\n",
    "                    print(\"üìù Contenido:\")\n",
    "                    print(last_msg.content[:300] if len(last_msg.content) > 300 else last_msg.content)\n",
    "                elif hasattr(last_msg, \"tool_calls\"):\n",
    "                    print(f\"üîß Tool calls: {[tc.name for tc in last_msg.tool_calls]}\")\n",
    "                else:\n",
    "                    pprint(last_msg)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Ejecuci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a41c8f",
   "metadata": {},
   "source": [
    "## 16. Ejecutar el grafo - Modo PRODUCCI√ìN\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-10 minutos**\n",
    "\n",
    "### Este modo:\n",
    "- Solo muestra la respuesta final\n",
    "- Modo limpio (sin debug)\n",
    "- Usa formateo Markdown para mejor visualizaci√≥n\n",
    "\n",
    "üéØ **Objetivo**: Resultado final en forma de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "222be27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Ejecutando el grafo en modo PRODUCCI√ìN...\n",
      "\n",
      "‚ùì Pregunta: Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue s√≥n los  flavours de MLFlow?\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚è≥ Evaluando relevancia de documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 28.77460441s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Score de relevancia: si\n",
      "‚úÖ Docs relevantes ‚Üí generando respuesta\n",
      "\n",
      "ü§ñ Generando respuesta final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 26.694575349s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 26\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 22.599006489s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 22\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 14.511296941s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\n",
      "Please retry in 58.424527442s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Respuesta generada: Los \"flavours\" de MLFlow son diferentes tipos o interfaces de modelos que MLFlow soporta. Permiten registrar, guardar, cargar y desplegar modelos de d...\n",
      "\n",
      "üìù RESPUESTA FINAL:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Los \"flavours\" de MLFlow son diferentes tipos o interfaces de modelos que MLFlow soporta. Permiten registrar, guardar, cargar y desplegar modelos de diversas plataformas como PyTorch, TensorFlow, LangChain, entre otras. El `mlflow.pyfunc` es un \"flavour\" predeterminado para modelos de Python, que sirve como una interfaz general para cargar y usar cualquier modelo de MLFlow Python como una funci√≥n."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Consulta completada\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"\\nüöÄ Ejecutando el grafo en modo PRODUCCI√ìN...\\n\")\n",
    "\n",
    "# Define tu pregunta\n",
    "pregunta = \"Busca en la informacion proporcionada la pregunta que hace el usuario y no inventes¬øQue s√≥n los  flavours de MLFlow?\"\n",
    "print(f\"‚ùì Pregunta: {pregunta}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejecuta el grafo y muestra solo la respuesta final\n",
    "resultado = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]})\n",
    "\n",
    "print(\"\\nüìù RESPUESTA FINAL:\\n\")\n",
    "display(Markdown(resultado[\"messages\"][-1].content))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Consulta completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcb813",
   "metadata": {},
   "source": [
    "## 17. Resumen y Conceptos Clave\n",
    "\n",
    "‚è±Ô∏è **Tiempo estimado: 5-7 minutos** (discusi√≥n/preguntas)\n",
    "\n",
    "### Flujo RAG Agentico completo:\n",
    "1. **Cargar** (secs 1-4): Documentos ‚Üí PDFs\n",
    "2. **Chunking** (sec 5): Documentos ‚Üí Chunks (piezas manejables)\n",
    "3. **Embeddings** (sec 6): Chunks ‚Üí Vectores (n√∫meros)\n",
    "4. **Nodo 1** (sec 8): ¬øBuscar o responder?\n",
    "   - Si buscar ‚Üí Nodo 2 (retrieve)\n",
    "   - Si responder ‚Üí END\n",
    "5. **Nodo 2** (sec 6-7): Buscar documentos similares\n",
    "6. **Nodo Evaluador** (sec 9): ¬øSon relevantes?\n",
    "   - Si relevantes ‚Üí Nodo 4 (responder)\n",
    "   - Si no ‚Üí Nodo 3 (reescribir pregunta)\n",
    "7. **Nodo 3** (sec 10): Mejorar pregunta ‚Üí Volver a Nodo 1\n",
    "8. **Nodo 4** (sec 11): Generar respuesta final ‚Üí END\n",
    "\n",
    "### Conceptos clave aprendidos:\n",
    "- **Embeddings**: Representaci√≥n num√©rica del texto\n",
    "- **Vector Store**: Base de datos de b√∫squeda sem√°ntica\n",
    "- **Agentic Loop**: Ciclo de decisi√≥n autom√°tico\n",
    "- **Retrieval Augmented Generation (RAG)**: Mejorar respuestas con contexto\n",
    "- **Graph State**: M√°quina de estados con LangGraph\n",
    "- **Tool Use**: Modelos pueden llamar herramientas autom√°ticamente\n",
    "\n",
    "### Tips para mejorar:\n",
    "1. **Tuning de chunks**: Experimenta con CHUNK_SIZE y CHUNK_OVERLAP\n",
    "2. **Mejor prompt engineering**: Mejorar GRADE_PROMPT y GENERATE_PROMPT\n",
    "3. **Multiple retrievers**: Combinar BM25 + Sem√°ntico\n",
    "4. **Caching**: Guardar embeddings calculados\n",
    "5. **Evaluaci√≥n**: Metrics como precision@k, NDCG\n",
    "\n",
    "### Siguientes pasos:\n",
    "- üîß Personaliza los prompts para tu caso de uso\n",
    "- üìä Mide la calidad de respuestas (F1-score, BLEU, etc)\n",
    "- üöÄ Despliega en producci√≥n con FastAPI/Streamlit\n",
    "- üìà Monitorea performance en tiempo real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
