{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a51931",
   "metadata": {},
   "source": [
    "## 1. Instalamos dependencias y librerias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2376e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q langchain>=0.2.5 langchain-community>=0.2.0 langchain-text-splitters>=0.2.0 langchain-google-genai>=0.0.10 chromadb>=0.5.0 tiktoken>=0.7.0 pypdf>=4 python-dotenv>=1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (GOOGLE_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat con Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e339c0d",
   "metadata": {},
   "source": [
    "## 2. Configuramos el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ee9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Cambia a tu carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs\")  # Carpeta donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = <FILL_IN>\n",
    "CHUNK_OVERLAP = <FILL_IN>\n",
    "TOP_K = 4\n",
    "\n",
    "# Modelos (ajusta si quieres otros)\n",
    "EMBEDDING_MODEL = \"<FILL_IN>\"\n",
    "CHAT_MODEL = \"<FILL_IN>\"\n",
    "\n",
    "# Verificar clave\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"Falta GOOGLE_API_KEY en variables de entorno o .env\"\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è Persistencia Chroma: {PERSIST_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df5afb",
   "metadata": {},
   "source": [
    "## 3. Cargamos los pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(pdf_paths, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos m√≠nimos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965b9c0",
   "metadata": {},
   "source": [
    "## 4. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300aecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=<FILL_IN>,\n",
    "    chunk_overlap=<FILL_IN>,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\" Chunks generados: {len(chunks)}\")\n",
    "chunks[:2]  # vista r√°pida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4756dfb",
   "metadata": {},
   "source": [
    "## 5. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=<FILL_IN>)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=<FILL_IN>,\n",
    "    embedding=<FILL_IN>,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "retriever =<FILL_IN>\n",
    "print(\" Chroma persistido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b6401",
   "metadata": {},
   "source": [
    "## 6. Creamos la Tool (Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    <FILL_IN>,\n",
    "    \"<FILL_IN>\",#nombre\n",
    "    \"<FILL_IN>\",#Descripcion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637af342",
   "metadata": {},
   "source": [
    "3 Test the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b974cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke({\"query\": \"<FILL_IN>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e098408",
   "metadata": {},
   "source": [
    "3. Generamos la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "response_model=  ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "\n",
    "def genera_query_o_responde(state: MessagesState):\n",
    "    \"\"\"Llama al modelo para generar una respuesta basada en el estado actual.\n",
    "      Dada la pregunta, decidir√° si recupera informaci√≥n usando la herramienta de recuperaci√≥n o simplemente responde al usuario.\"\"\"\n",
    "   \n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([<FILL_IN>]).invoke(state[<FILL_IN>])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ead40",
   "metadata": {},
   "source": [
    "3.1 Lo probamos con una pregunta aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c696202",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"<FILL_IN>\"}]}\n",
    "respuesta= genera_query_o_responde(input)\n",
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cogemos el ultimo mensaje\n",
    "respuesta[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b79f09",
   "metadata": {},
   "source": [
    "3.2  Hacer una pregunta que requiera una busqueda semantica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"<FILL_IN>\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "genera_query_o_responde(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2cf6f",
   "metadata": {},
   "source": [
    "4. Vemos la relevancia del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"Eres un evaluador que determina la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \"\n",
    "    \"Aqu√≠ tienes el documento recuperado: \\n\\n {context} \\n\\n\"\n",
    "    \"Aqu√≠ tienes la pregunta del usuario: {question} \\n\"\n",
    "    \"Si el documento contiene palabra(s) clave o significado sem√°ntico relacionado con la pregunta del usuario, calif√≠calo como relevante. \\n\"\n",
    "    \"Da una puntuaci√≥n binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Califica los documentos utilizando una puntuaci√≥n binaria para comprobar su relevancia\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Puntuaci√≥n : 'si' si es relevante, o 'no' si no lo es\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"genera_respuesta\", \"rescribir_question\"]:\n",
    "    print(\"grade_documents\")\n",
    "    \"\"\"Determina si los documentos recuperados son relevantes para la pregunta.\"\"\"\n",
    "    question = <FILL_IN>\n",
    "    context = <FILL_IN>\n",
    "    \n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "    print (\"score :\", score)\n",
    "    if score == \"si\":\n",
    "        return \"genera_respuesta\"\n",
    "    else:\n",
    "        return \"rescribir_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd6113",
   "metadata": {},
   "source": [
    "4.1 Ejecutar con una respuesta irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccf626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulamos la respuesta de la tool mediante mensajes\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"<FILL_IN>\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"<FILL_IN>\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9278fb6",
   "metadata": {},
   "source": [
    "4.2 Coomprobar que el documento/respuesta relevante lo clasifica como tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"<FILL_IN>\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f93eb",
   "metadata": {},
   "source": [
    "5 Rescribir la pregunta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef12b9",
   "metadata": {},
   "source": [
    "5.1 Crear el nodo de reesctirura de la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Analiza detenidamente la siguiente pregunta e intenta comprender la intenci√≥n o el significado profundo que transmite.\\n\"\n",
    "    \"Pregunta original:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Ahora, reescribe la pregunta para que sea m√°s clara, precisa y f√°cil de entender:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rescribir_question(state: MessagesState):\n",
    "    \"\"\"Rescribe/Mejora la pregunta original del usuario.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = <FILL_IN>\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e92990",
   "metadata": {},
   "source": [
    "5.2 Probamos  rescribir la pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cdbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"<FILL_IN>\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"<FILL_IN>\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rescribir_question(input)\n",
    "print(response[\"messages\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6927d76",
   "metadata": {},
   "source": [
    "## 6. Generamos la Respuesta.\n",
    "\n",
    "### 6.1. Construimos el nodo generate_answer.\n",
    "Si superamos las comprobaciones del evaluador (grader), podemos generar la respuesta final bas√°ndonos en la pregunta original y el contexto recuperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"Eres un asistente para tareas de preguntas y respuestas. \"\n",
    "    \"Utiliza los siguientes fragmentos de contexto recuperado para responder a la pregunta. \"\n",
    "    \"Si no sabes la respuesta, simplemente indica que no la sabes. \"\n",
    "    \"Utiliza un m√°ximo de tres frases y mant√©n la respuesta concisa.\\n\"\n",
    "    \"Pregunta: {question} \\n\"\n",
    "    \"Contexto: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def genera_respuesta(state: MessagesState):\n",
    "    \"\"\"Genera la respuesta.\"\"\"\n",
    "    print(\"genera_respuesta\")\n",
    "    question = <FILL_IN>\n",
    "    context = <FILL_IN>\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef55195",
   "metadata": {},
   "source": [
    "### 6.2. Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5999dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"<FILL_IN>\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"<FILL_IN>\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = genera_respuesta(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4374e3",
   "metadata": {},
   "source": [
    "## 7. Configurar el grafo.\n",
    "### 7.1 Importamos los elementos necesarios para construir el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hasta aqui ya tenemos todo el procesamiento y necesitamos \n",
    "#importar los elementos necesarios para construir el grafo. Xavi\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb409008",
   "metadata": {},
   "source": [
    "### 7.2 Ensamblamos el workflow\n",
    "#### 7.2.1 A√±adimos los nodos\n",
    "![grafo_nodos.jpg](attachment:d4922386-7270-40c8-b5a2-113b13c5be37.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(<FILL_IN>)\n",
    "workflow.add_node(<FILL_IN>)\n",
    "workflow.add_node(\"retrieve\", ToolNode([<FILL_IN>]))\n",
    "workflow.add_node(<FILL_IN>)\n",
    "workflow.add_node(<FILL_IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1298a3",
   "metadata": {},
   "source": [
    "#### 7.2.1 A√±adimos las aristas\n",
    "![grafo_aristas.jpg](attachment:4023440b-77e0-42fa-b06b-15d0c62f5714.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge(<FILL_IN>, \"genera_query_o_responde\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"genera_query_o_responde\",\n",
    "    # Eval√∫a la decisi√≥n del LLM (llama a la herramienta retriever_tool o responde al usuario)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Traduce las salidas de la condici√≥n a nodos dentro de nuestro grafo\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "#\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Eval√∫a la decisi√≥n del agente con la funcion de condicion\n",
    "    <FILL_IN>,  \n",
    ")\n",
    "\n",
    "workflow.add_edge(\"genera_respuesta\", <FILL_IN>)\n",
    "\n",
    "workflow.add_edge(\"rescribir_question\", \"genera_query_o_responde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7485e0",
   "metadata": {},
   "source": [
    "#### 7.2.3 Compilamos el Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aceba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717bb630",
   "metadata": {},
   "source": [
    "#### 7.2.4 Pintamos el Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27357d",
   "metadata": {},
   "source": [
    "#### 7.4 Ver como funciona el grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint  # para imprimir bonito\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"<FILL_IN>\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"üìò Update from node: {node}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        messages = update.get(\"messages\", [])\n",
    "      \n",
    "        last_msg = messages[-1]\n",
    "\n",
    "        try:\n",
    "            if isinstance(last_msg, dict):\n",
    "                if \"content\" in last_msg:\n",
    "                    print(\"üìù Contenido textual:\")\n",
    "                    print(last_msg[\"content\"])\n",
    "                elif \"tool_calls\" in last_msg:\n",
    "                    print(\"üîß Llamada a funci√≥n:\")\n",
    "                    pprint(last_msg[\"tool_calls\"])\n",
    "                else:\n",
    "                    print(\"üïµÔ∏è Mensaje dict sin content/tool_calls:\")\n",
    "                    pprint(last_msg)\n",
    "            elif hasattr(last_msg, \"content\"):\n",
    "                print(\"üìù Contenido desde objeto:\")\n",
    "                print(last_msg.content)\n",
    "            else:\n",
    "                print(\"üïµÔ∏è Mensaje desconocido:\")\n",
    "                pprint(last_msg)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error leyendo el mensaje:\", str(e))\n",
    "            pprint(last_msg)\n",
    "\n",
    "        print(\"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b8405",
   "metadata": {},
   "source": [
    "#### 7.5 Hacer una ejecuci√≥n del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f236bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"<FILL_IN>\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(graph.invoke(input)[\"messages\"][-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15bc583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
