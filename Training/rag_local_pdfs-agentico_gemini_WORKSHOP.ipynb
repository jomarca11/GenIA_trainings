{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298d5b29",
   "metadata": {},
   "source": [
    "## 1. Instalamos dependencias y librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q langchain>=0.2.5 langchain-community>=0.2.0 langchain-text-splitters>=0.2.0 langchain-google-genai>=0.0.10 chromadb>=0.5.0 tiktoken>=0.7.0 pypdf>=4 python-dotenv>=1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada62beb",
   "metadata": {},
   "source": [
    "## 2. Importamos las librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c68db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Carga variables de entorno (GOOGLE_API_KEY, etc.)\n",
    "\n",
    "# LangChain loaders, splitters, vectorstore, LLM/embeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings y modelo de chat con Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Utilidad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c8c5e",
   "metadata": {},
   "source": [
    "## 3. Configuramos el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b518eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Configuraci√≥n\n",
    "PDF_DIR = Path(\"./docs\")             # <- Carpeta con PDFs\n",
    "PERSIST_DIR = Path(\"./chroma_pdfs_gemini\")  # Donde se guardar√° Chroma\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# <FILL_IN> Completa estos valores\n",
    "CHUNK_SIZE = <FILL_IN>  # Tama√±o de cada chunk (trozo) de texto\n",
    "CHUNK_OVERLAP = <FILL_IN>  # Solapamiento entre chunks\n",
    "TOP_K = 4  # N√∫mero de documentos recuperados\n",
    "\n",
    "# Modelos de Gemini\n",
    "EMBEDDING_MODEL = \"<FILL_IN>\"  # Modelo de embeddings (ej: \"models/text-embedding-004\")\n",
    "CHAT_MODEL = \"<FILL_IN>\"  # Modelo de chat (ej: \"gemini-2.5-flash\")\n",
    "\n",
    "# Verificar clave\n",
    "assert os.getenv(\"GOOGLE_API_KEY\"), \"Falta GOOGLE_API_KEY en variables de entorno o .env\"\n",
    "print(f\"üìÅ Carpeta PDFs: {PDF_DIR.resolve()}\")\n",
    "print(f\"üóÇÔ∏è Persistencia Chroma: {PERSIST_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670fbb5",
   "metadata": {},
   "source": [
    "## 4. Cargamos los PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_dir(directory: Path, recursive: bool = True):\n",
    "    \"\"\"Carga todos los PDFs de una carpeta de forma recursiva.\"\"\"\n",
    "    pattern = \"**/*.pdf\" if recursive else \"*.pdf\"\n",
    "    pdf_paths = sorted([p for p in directory.glob(pattern) if p.is_file()])\n",
    "    all_docs = []\n",
    "    for pdf in tqdm(pdf_paths, desc=\"Cargando PDFs\"):\n",
    "        try:\n",
    "            docs = PyPDFLoader(str(pdf)).load()\n",
    "            # A√±adimos metadatos √∫tiles\n",
    "            for d in docs:\n",
    "                d.metadata = d.metadata or {}\n",
    "                d.metadata[\"source\"] = str(pdf.resolve())\n",
    "            all_docs.extend(docs)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {pdf}: {e}\")\n",
    "    print(f\"üìö Documentos (p√°ginas) cargados: {len(all_docs)}\")\n",
    "    return all_docs\n",
    "\n",
    "raw_docs = load_pdfs_from_dir(PDF_DIR, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665b280",
   "metadata": {},
   "source": [
    "## 5. Chunking (Trozamos los documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c86eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Crea el splitter con los par√°metros configurados arriba\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=<FILL_IN>,\n",
    "    chunk_overlap=<FILL_IN>,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f\"‚úÇÔ∏è Chunks generados: {len(chunks)}\")\n",
    "print(f\"üìù Ejemplo de primer chunk:\\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52335f",
   "metadata": {},
   "source": [
    "## 6. Crear Embeddings y Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa307b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Crea los embeddings con Gemini\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=<FILL_IN>)\n",
    "\n",
    "# <FILL_IN> Crea el vector store con Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=<FILL_IN>,\n",
    "    embedding=<FILL_IN>,\n",
    "    persist_directory=str(PERSIST_DIR),\n",
    ")\n",
    "\n",
    "# <FILL_IN> Crea el retriever\n",
    "retriever = <FILL_IN>\n",
    "print(\"‚úÖ Chroma persistido y retriever creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c0437",
   "metadata": {},
   "source": [
    "## 7. Creamos la Tool (Retriever) para el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# <FILL_IN> Crea la retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    <FILL_IN>,  # retriever\n",
    "    \"<FILL_IN>\",  # nombre de la tool\n",
    "    \"<FILL_IN>\",  # descripci√≥n\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever tool creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a77a63c",
   "metadata": {},
   "source": [
    "### 7.1 Test de la tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba la tool con una pregunta de ejemplo\n",
    "resultado = retriever_tool.invoke({\"query\": \"<FILL_IN>\"})\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627ae19",
   "metadata": {},
   "source": [
    "## 8. Nodo: Genera query o responde directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f91b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# <FILL_IN> Instancia el modelo de chat\n",
    "response_model = ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "\n",
    "def genera_query_o_responde(state: MessagesState):\n",
    "    \"\"\"Decide si recuperar informaci√≥n o responder directamente.\"\"\"\n",
    "    # <FILL_IN> Usa bind_tools para conectar la retriever_tool\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([<FILL_IN>]).invoke(state[<FILL_IN>])\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d3404",
   "metadata": {},
   "source": [
    "### 8.1 Test: Pregunta que no necesita b√∫squeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba con una pregunta que no necesita b√∫squeda\n",
    "input_test = {\"messages\": [{\"role\": \"user\", \"content\": \"<FILL_IN>\"}]}\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903e525",
   "metadata": {},
   "source": [
    "### 8.2 Test: Pregunta que requiere b√∫squeda sem√°ntica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f125ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Prueba con una pregunta que requiere b√∫squeda\n",
    "input_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"<FILL_IN>\",  # Una pregunta sobre tus PDFs\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "respuesta = genera_query_o_responde(input_test)\n",
    "respuesta[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e8d4d",
   "metadata": {},
   "source": [
    "## 9. Nodo: Evaluar relevancia de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"Eres un evaluador que determina la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \"\n",
    "    \"Aqu√≠ tienes el documento recuperado: \\n\\n {context} \\n\\n\"\n",
    "    \"Aqu√≠ tienes la pregunta del usuario: {question} \\n\"\n",
    "    \"Si el documento contiene palabra(s) clave o significado sem√°ntico relacionado con la pregunta del usuario, calif√≠calo como relevante. \\n\"\n",
    "    \"Da una puntuaci√≥n binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Califica los documentos utilizando una puntuaci√≥n binaria\"\"\"\n",
    "    binary_score: str = Field(description=\"Puntuaci√≥n: 'si' si es relevante, o 'no' si no lo es\")\n",
    "\n",
    "\n",
    "# <FILL_IN> Instancia el modelo grader\n",
    "grader_model = ChatGoogleGenerativeAI(model=<FILL_IN>, temperature=<FILL_IN>)\n",
    "\n",
    "\n",
    "def grade_documents(state: MessagesState) -> Literal[\"genera_respuesta\", \"rescribir_question\"]:\n",
    "    \"\"\"Determina si los documentos recuperados son relevantes.\"\"\"\n",
    "    print(\"‚è≥ Evaluando relevancia de documentos...\")\n",
    "    # <FILL_IN> Extrae la pregunta y el contexto\n",
    "    question = <FILL_IN>\n",
    "    context = <FILL_IN>\n",
    "    \n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "    print(f\"üìä Score de relevancia: {score}\")\n",
    "    \n",
    "    if score == \"si\":\n",
    "        return \"genera_respuesta\"\n",
    "    else:\n",
    "        return \"rescribir_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd2a0a",
   "metadata": {},
   "source": [
    "## 10. Nodo: Rescribir la pregunta si no es clara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d70c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Analiza detenidamente la siguiente pregunta e intenta comprender la intenci√≥n o el significado profundo que transmite.\\n\"\n",
    "    \"Pregunta original:\\n ------- \\n{question}\\n ------- \\n\"\n",
    "    \"Ahora, reescribe la pregunta para que sea m√°s clara, precisa y f√°cil de entender:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rescribir_question(state: MessagesState):\n",
    "    \"\"\"Reescribe la pregunta del usuario para mejorarla.\"\"\"\n",
    "    print(\"‚úèÔ∏è Reescribiendo pregunta...\")\n",
    "    messages = state[\"messages\"]\n",
    "    # <FILL_IN> Extrae la pregunta original\n",
    "    question = <FILL_IN>\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd48b18",
   "metadata": {},
   "source": [
    "## 11. Nodo: Generar la respuesta final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"Eres un asistente para tareas de preguntas y respuestas. \"\n",
    "    \"Utiliza los siguientes fragmentos de contexto recuperado para responder a la pregunta. \"\n",
    "    \"Si no sabes la respuesta, simplemente indica que no la sabes. \"\n",
    "    \"Utiliza un m√°ximo de tres frases y mant√©n la respuesta concisa.\\n\"\n",
    "    \"Pregunta: {question} \\n\"\n",
    "    \"Contexto: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def genera_respuesta(state: MessagesState):\n",
    "    \"\"\"Genera la respuesta final basada en el contexto.\"\"\"\n",
    "    print(\"ü§ñ Generando respuesta...\")\n",
    "    # <FILL_IN> Extrae la pregunta y el contexto\n",
    "    question = <FILL_IN>\n",
    "    context = <FILL_IN>\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eca855",
   "metadata": {},
   "source": [
    "## 12. Construir el grafo (workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# <FILL_IN> Crea el StateGraph\n",
    "workflow = StateGraph(<FILL_IN>)\n",
    "\n",
    "# <FILL_IN> A√±ade los nodos\n",
    "workflow.add_node(<FILL_IN>)  # genera_query_o_responde\n",
    "workflow.add_node(\"retrieve\", ToolNode([<FILL_IN>]))\n",
    "workflow.add_node(<FILL_IN>)  # rescribir_question\n",
    "workflow.add_node(<FILL_IN>)  # genera_respuesta\n",
    "\n",
    "# <FILL_IN> A√±ade las aristas\n",
    "workflow.add_edge(<FILL_IN>, \"genera_query_o_responde\")  # START a genera_query_o_responde\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"genera_query_o_responde\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    <FILL_IN>,  # grade_documents\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"genera_respuesta\", <FILL_IN>)  # END\n",
    "workflow.add_edge(\"rescribir_question\", \"genera_query_o_responde\")\n",
    "\n",
    "print(\"‚úÖ Grafo construido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0c0cf",
   "metadata": {},
   "source": [
    "## 13. Compilar el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <FILL_IN> Compila el grafo\n",
    "graph = <FILL_IN>\n",
    "print(\"‚úÖ Grafo compilado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab393e7",
   "metadata": {},
   "source": [
    "## 14. Visualizar el grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# <FILL_IN> Visualiza el grafo\n",
    "display(Image(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36fc041",
   "metadata": {},
   "source": [
    "## 15. Ejecutar el grafo - Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3be477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# <FILL_IN> Define tu pregunta\n",
    "pregunta = \"<FILL_IN>\"\n",
    "\n",
    "# Ejecuta el grafo\n",
    "for chunk in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]}):\n",
    "    for node, update in chunk.items():\n",
    "        print(f\"üìò Update from node: {node}\")\n",
    "        print(\"-\" * 40)\n",
    "        messages = update.get(\"messages\", [])\n",
    "        last_msg = messages[-1]\n",
    "        try:\n",
    "            if hasattr(last_msg, \"content\"):\n",
    "                print(\"üìù Contenido:\")\n",
    "                print(last_msg.content)\n",
    "            else:\n",
    "                pprint(last_msg)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32babf2",
   "metadata": {},
   "source": [
    "## 16. Ejecutar el grafo - Resultado Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f556ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# <FILL_IN> Define tu pregunta\n",
    "pregunta = \"<FILL_IN>\"\n",
    "\n",
    "# Ejecuta el grafo y muestra solo la respuesta final\n",
    "resultado = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": pregunta}]})\n",
    "display(Markdown(resultado[\"messages\"][-1].content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
